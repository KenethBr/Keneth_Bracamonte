{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4eae556",
   "metadata": {},
   "source": [
    "Paso 1: Carga del Dataset\n",
    "1. Importar Pandas y NumPy: Son las librer√≠as fundamentales para manejar datos (pandas) y realizar c√°lculos num√©ricos eficientes (numpy).\n",
    "\n",
    "2. Cargar el Archivo: Usamos pd.read_csv() para leer el archivo del AI4I 2020 Predictive Maintenance Dataset. Asumiremos que el archivo se llama ai4i2020.csv y se encuentra en la misma carpeta que tu Jupyter Notebook.\n",
    "\n",
    "3. Mostrar Encabezado: Usamos .head() para ver las primeras 10 filas y confirmar que la carga fue exitosa y para empezar a inspeccionar el formato de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ab87ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UDI</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>Air temperature [K]</th>\n",
       "      <th>Process temperature [K]</th>\n",
       "      <th>Rotational speed [rpm]</th>\n",
       "      <th>Torque [Nm]</th>\n",
       "      <th>Tool wear [min]</th>\n",
       "      <th>Machine failure</th>\n",
       "      <th>TWF</th>\n",
       "      <th>HDF</th>\n",
       "      <th>PWF</th>\n",
       "      <th>OSF</th>\n",
       "      <th>RNF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>M14860</td>\n",
       "      <td>M</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1551</td>\n",
       "      <td>42.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>L47181</td>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408</td>\n",
       "      <td>46.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>L47182</td>\n",
       "      <td>L</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.5</td>\n",
       "      <td>1498</td>\n",
       "      <td>49.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>L47183</td>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1433</td>\n",
       "      <td>39.5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>L47184</td>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408</td>\n",
       "      <td>40.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>M14865</td>\n",
       "      <td>M</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1425</td>\n",
       "      <td>41.9</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>L47186</td>\n",
       "      <td>L</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1558</td>\n",
       "      <td>42.4</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>L47187</td>\n",
       "      <td>L</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1527</td>\n",
       "      <td>40.2</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>M14868</td>\n",
       "      <td>M</td>\n",
       "      <td>298.3</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1667</td>\n",
       "      <td>28.6</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>M14869</td>\n",
       "      <td>M</td>\n",
       "      <td>298.5</td>\n",
       "      <td>309.0</td>\n",
       "      <td>1741</td>\n",
       "      <td>28.0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UDI     Product ID   Type   Air temperature [K]   Process temperature [K]  \\\n",
       "0      1   M14860       M                    298.1                     308.6   \n",
       "1      2   L47181       L                    298.2                     308.7   \n",
       "2      3   L47182       L                    298.1                     308.5   \n",
       "3      4   L47183       L                    298.2                     308.6   \n",
       "4      5   L47184       L                    298.2                     308.7   \n",
       "5      6   M14865       M                    298.1                     308.6   \n",
       "6      7   L47186       L                    298.1                     308.6   \n",
       "7      8   L47187       L                    298.1                     308.6   \n",
       "8      9   M14868       M                    298.3                     308.7   \n",
       "9     10   M14869       M                    298.5                     309.0   \n",
       "\n",
       "    Rotational speed [rpm]   Torque [Nm]   Tool wear [min]   Machine failure  \\\n",
       "0                     1551          42.8                 0                 0   \n",
       "1                     1408          46.3                 3                 0   \n",
       "2                     1498          49.4                 5                 0   \n",
       "3                     1433          39.5                 7                 0   \n",
       "4                     1408          40.0                 9                 0   \n",
       "5                     1425          41.9                11                 0   \n",
       "6                     1558          42.4                14                 0   \n",
       "7                     1527          40.2                16                 0   \n",
       "8                     1667          28.6                18                 0   \n",
       "9                     1741          28.0                21                 0   \n",
       "\n",
       "    TWF   HDF   PWF   OSF   RNF  \n",
       "0     0     0     0     0     0  \n",
       "1     0     0     0     0     0  \n",
       "2     0     0     0     0     0  \n",
       "3     0     0     0     0     0  \n",
       "4     0     0     0     0     0  \n",
       "5     0     0     0     0     0  \n",
       "6     0     0     0     0     0  \n",
       "7     0     0     0     0     0  \n",
       "8     0     0     0     0     0  \n",
       "9     0     0     0     0     0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"./ai4i2020.csv\")\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9060bc8",
   "metadata": {},
   "source": [
    "Paso 2: Limpieza de nombres de columnas y An√°lisis Exploratorio de Datos (EDA)\n",
    "1. Crea una herramienta que toma un nombre de columna, elimina espacios al inicio/final, y reemplaza secuencias de m√∫ltiples espacios con un solo espacio.\n",
    "2. df.info(): Imprime un resumen de cu√°ntos valores no nulos hay en cada columna y el tipo de dato (Dtype).\n",
    "3. df.describe(): Calcula estad√≠sticas b√°sicas (media, min, max, desviaci√≥n est√°ndar) para las columnas num√©ricas.\n",
    "4. df[NOMBRE_OBJETIVO].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b3712ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2.2.1 Informaci√≥n General del DataFrame ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   UDI                      10000 non-null  int64  \n",
      " 1   Product ID               10000 non-null  object \n",
      " 2   Type                     10000 non-null  object \n",
      " 3   Air temperature [K]      10000 non-null  float64\n",
      " 4   Process temperature [K]  10000 non-null  float64\n",
      " 5   Rotational speed [rpm]   10000 non-null  int64  \n",
      " 6   Torque [Nm]              10000 non-null  float64\n",
      " 7   Tool wear [min]          10000 non-null  int64  \n",
      " 8   Machine failure          10000 non-null  int64  \n",
      " 9   TWF                      10000 non-null  int64  \n",
      " 10  HDF                      10000 non-null  int64  \n",
      " 11  PWF                      10000 non-null  int64  \n",
      " 12  OSF                      10000 non-null  int64  \n",
      " 13  RNF                      10000 non-null  int64  \n",
      "dtypes: float64(3), int64(9), object(2)\n",
      "memory usage: 1.1+ MB\n",
      "\n",
      "--- 2.2.2 Estad√≠sticas Descriptivas de Variables Num√©ricas ---\n",
      "               UDI  Air temperature [K]  Process temperature [K]  \\\n",
      "count  10000.00000         10000.000000             10000.000000   \n",
      "mean    5000.50000           300.004930               310.005560   \n",
      "std     2886.89568             2.000259                 1.483734   \n",
      "min        1.00000           295.300000               305.700000   \n",
      "25%     2500.75000           298.300000               308.800000   \n",
      "50%     5000.50000           300.100000               310.100000   \n",
      "75%     7500.25000           301.500000               311.100000   \n",
      "max    10000.00000           304.500000               313.800000   \n",
      "\n",
      "       Rotational speed [rpm]   Torque [Nm]  Tool wear [min]  Machine failure  \\\n",
      "count            10000.000000  10000.000000     10000.000000     10000.000000   \n",
      "mean              1538.776100     39.986910       107.951000         0.033900   \n",
      "std                179.284096      9.968934        63.654147         0.180981   \n",
      "min               1168.000000      3.800000         0.000000         0.000000   \n",
      "25%               1423.000000     33.200000        53.000000         0.000000   \n",
      "50%               1503.000000     40.100000       108.000000         0.000000   \n",
      "75%               1612.000000     46.800000       162.000000         0.000000   \n",
      "max               2886.000000     76.600000       253.000000         1.000000   \n",
      "\n",
      "                TWF           HDF           PWF           OSF          RNF  \n",
      "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.00000  \n",
      "mean       0.004600      0.011500      0.009500      0.009800      0.00190  \n",
      "std        0.067671      0.106625      0.097009      0.098514      0.04355  \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.00000  \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.00000  \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.00000  \n",
      "75%        0.000000      0.000000      0.000000      0.000000      0.00000  \n",
      "max        1.000000      1.000000      1.000000      1.000000      1.00000  \n",
      "\n",
      "--- 2.3.1 Conteo de la Variable Objetivo ('Machine failure') ---\n",
      "Machine failure\n",
      "0    9661\n",
      "1     339\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Porcentaje de Fallo de M√°quina (Clase 1): 3.39%\n",
      "Este bajo porcentaje confirma un problema de desbalance de clases.\n"
     ]
    }
   ],
   "source": [
    "# Funci√≥n para limpiar nombres: reemplaza m√∫ltiples espacios por uno y elimina espacios al inicio/final\n",
    "def limpiar_nombre_columna(col):\n",
    "    # ' '.join(col.split()) elimina cualquier secuencia de espacios y la reemplaza por un solo espacio.\n",
    "    col = ' '.join(col.split())\n",
    "    return col.strip()\n",
    "\n",
    "# Aplicar la limpieza a todos los nombres de columna\n",
    "df.columns = [limpiar_nombre_columna(col) for col in df.columns]\n",
    "\"\"\"\"\n",
    "print(\"--- Nombres de columnas despu√©s de la limpieza ---\")\n",
    "print(df.columns.tolist())\n",
    "\"\"\"\n",
    "\n",
    "# --- 2.2 AN√ÅLISIS DE TIPOS DE DATOS Y ESTAD√çSTICAS ---\n",
    "\n",
    "print(\"\\n--- 2.2.1 Informaci√≥n General del DataFrame ---\")\n",
    "# Muestra el tipo de dato y la cuenta de valores no nulos (verificar si hay NaNs)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n--- 2.2.2 Estad√≠sticas Descriptivas de Variables Num√©ricas ---\")\n",
    "# Muestra estad√≠sticas como media, desviaci√≥n est√°ndar, min/max.\n",
    "print(df.describe())\n",
    "\n",
    "\n",
    "# --- 2.3 AN√ÅLISIS DE LA VARIABLE OBJETIVO ---\n",
    "\n",
    "print(f\"\\n--- 2.3.1 Conteo de la Variable Objetivo ('Machine failure') ---\")\n",
    "# Analiza el balance de la variable objetivo (0 = No Fallo, 1 = Fallo)\n",
    "conteo_fallos = df['Machine failure'].value_counts()\n",
    "print(conteo_fallos)\n",
    "\n",
    "# Calcular el porcentaje de fallos\n",
    "porcentaje_fallos = (conteo_fallos.get(1, 0) / df.shape[0]) * 100\n",
    "print(f\"\\nPorcentaje de Fallo de M√°quina (Clase 1): {porcentaje_fallos:.2f}%\")\n",
    "print(\"Este bajo porcentaje confirma un problema de desbalance de clases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f293b0",
   "metadata": {},
   "source": [
    "Paso 3: Preprocesamiento de Datos y Escalado\n",
    "\n",
    "1. Eliminar Columnas Innecesarias: Se eliminan las columnas de identificaci√≥n (UDI, Product ID) que no tienen poder predictivo.\n",
    "\n",
    "2. Separar X e y: El dataset se divide en caracter√≠sticas predictoras (X) y la variable objetivo (y), que es 'Machine failure'.\n",
    "\n",
    "3. Codificaci√≥n de Variable Categ√≥rica: Se aplica One-Hot Encoding a la columna 'Type' (L, M, H) para convertirla en columnas binarias (ej: Type_L, Type_M), ya que los modelos matem√°ticos solo trabajan con n√∫meros.\n",
    "\n",
    "4. Identificar Num√©ricas: Se identifican las columnas con valores continuos (temperatura, velocidad, torque, etc.).\n",
    "\n",
    "5. Escalado (StandardScaler): Se aplica StandardScaler a las columnas num√©ricas. Esto es crucial: centra los datos en una media de 0 y desviaci√≥n est√°ndar de 1, igualando las escalas para que Naive Bayes y KDE funcionen correctamente.\n",
    "\n",
    "6. Divisi√≥n de Datos (70/30): El dataset se divide en un 70% para entrenamiento (X_train, y_train) y un 30% para prueba (X_test, y_test). La opci√≥n stratify=y asegura que la proporci√≥n de fallos (el desbalance) se mantenga igual en ambos conjuntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "640797b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas eliminadas: ['UDI', 'Product ID', 'TWF', 'HDF', 'PWF', 'OSF', 'RNF']\n",
      "Columnas finales de X: ['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]', 'Type_ L   ', 'Type_ M   ']\n",
      "\n",
      "--- Resumen de la Divisi√≥n ---\n",
      "Tama√±o de Entrenamiento (X_train): (7000, 7)\n",
      "Tama√±o de Prueba (X_test): (3000, 7)\n"
     ]
    }
   ],
   "source": [
    "# Importar librer√≠as necesarias\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Volver a cargar el dataframe original y limpiar nombres\n",
    "df = pd.read_csv(\"./ai4i2020.csv\") # Aseg√∫rate de que esta ruta sea correcta\n",
    "def limpiar_nombre_columna(col):\n",
    "    col = ' '.join(col.split())\n",
    "    return col.strip()\n",
    "df.columns = [limpiar_nombre_columna(col) for col in df.columns]\n",
    "NOMBRE_OBJETIVO = 'Machine failure'\n",
    "\n",
    "# 3.1 Columnas a eliminar AHORA:\n",
    "columnas_a_eliminar = [\n",
    "    'UDI', \n",
    "    'Product ID', \n",
    "    # ¬°NUEVA CORRECCI√ìN! Eliminar las causas binarias de fallo\n",
    "    'TWF', 'HDF', 'PWF', 'OSF', 'RNF'\n",
    "]\n",
    "df = df.drop(columns=columnas_a_eliminar)\n",
    "print(f\"Columnas eliminadas: {columnas_a_eliminar}\")\n",
    "\n",
    "\n",
    "# 3.2 Separar X (predictoras) e y (objetivo)\n",
    "X = df.drop(columns=[NOMBRE_OBJETIVO])\n",
    "y = df[NOMBRE_OBJETIVO]\n",
    "\n",
    "\n",
    "# 3.3 Codificaci√≥n de Variable Categ√≥rica ('Type')\n",
    "# Se mantiene drop_first=True para evitar colinealidad (corregida previamente)\n",
    "X = pd.get_dummies(X, columns=['Type'], drop_first=True) \n",
    "print(f\"Columnas finales de X: {X.columns.tolist()}\")\n",
    "\n",
    "\n",
    "# 3.4 Identificar y Escalar columnas num√©ricas\n",
    "columnas_numericas = X.select_dtypes(include=np.number).columns.tolist()\n",
    "scaler = StandardScaler()\n",
    "X[columnas_numericas] = scaler.fit_transform(X[columnas_numericas])\n",
    "\n",
    "\n",
    "# 3.5 Divisi√≥n en conjuntos de entrenamiento y prueba (70/30)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\n--- Resumen de la Divisi√≥n ---\")\n",
    "print(f\"Tama√±o de Entrenamiento (X_train): {X_train.shape}\")\n",
    "print(f\"Tama√±o de Prueba (X_test): {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a1dee4",
   "metadata": {},
   "source": [
    "Paso 4: Establecer la L√≠nea Base (IA 1 - GaussianNB)\n",
    "\n",
    "Este paso implementa tu primera IA (IA 1), que es el modelo Gaussian Naive Bayes est√°ndar de Scikit-learn. Este modelo act√∫a como el punto de referencia (baseline) con el cual comparar√°s tus implementaciones de Naive Bayes con KDE m√°s adelante.\n",
    "\n",
    "1. Importar Herramientas: Importar las clases necesarias: GaussianNB (el modelo) y roc_auc_score (la m√©trica requerida en tu metodolog√≠a).\n",
    "\n",
    "2. Instanciar y Entrenar: Se inicializa el modelo GaussianNB y se entrena usando el conjunto de entrenamiento (X_train, y_train) que preparamos en el Paso 3.\n",
    "\n",
    "3. Predicci√≥n de Probabilidades: Se utiliza el modelo para predecir las probabilidades de pertenencia a la clase (0 o 1) en el conjunto de prueba (X_test). La m√©trica AUC ROC requiere probabilidades, no solo la clase final.\n",
    "\n",
    "4. Evaluaci√≥n: Se calcula el valor AUC ROC usando las probabilidades predichas y los valores reales (y_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de64cbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluaci√≥n de la L√≠nea Base (Gaussian Naive Bayes) ---\n",
      "AUC ROC del Baseline (GaussianNB): 0.8626\n"
     ]
    }
   ],
   "source": [
    "# 4.1 Importar el modelo y la m√©trica de evaluaci√≥n\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# 4.2 Instanciar y Entrenar el modelo\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X_train, y_train)\n",
    "\n",
    "# 4.3 Predecir las probabilidades en el conjunto de prueba\n",
    "# Usamos predict_proba para obtener la probabilidad de que sea la clase 1 (fallo)\n",
    "y_pred_proba_gnb = gnb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 4.4 Evaluar el modelo con la m√©trica AUC ROC\n",
    "auc_roc_gnb = roc_auc_score(y_test, y_pred_proba_gnb)\n",
    "\n",
    "print(\"--- Evaluaci√≥n de la L√≠nea Base (Gaussian Naive Bayes) ---\")\n",
    "print(f\"AUC ROC del Baseline (GaussianNB): {auc_roc_gnb:.4f}\")\n",
    "\n",
    "# Guardamos el resultado del baseline para compararlo en pasos futuros\n",
    "resultados = {'GaussianNB_Baseline': auc_roc_gnb}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbf5ac2",
   "metadata": {},
   "source": [
    "Paso 5: Implementaci√≥n del Clasificador Naive Bayes Personalizado\n",
    "\n",
    "Este c√≥digo implementa la l√≥gica de Naive Bayes utilizando el logaritmo de las probabilidades para evitar errores de underflow (probabilidades muy peque√±as). Usaremos KernelDensity de Scikit-learn como la herramienta para la estimaci√≥n.\n",
    "\n",
    "1. Definir la Clase: Creamos una clase CustomNaiveBayesKDE con un m√©todo de inicializaci√≥n (__init__) para almacenar el m√©todo KDE y el bandwidth $h$ que se usar√°n.\n",
    "2. M√©todo fit(X, y) (Entrenamiento):\n",
    "- Calcular Probabilidad a Priori: Calcula $P(y)$, la probabilidad de cada clase (Fallo y No Fallo).\n",
    "- Entrenar KDE: Para cada caracter√≠stica $x_i$ y para cada clase $y$, ajusta el estimador de densidad (KDE) a los datos de entrenamiento. Estos modelos ajustados se almacenan.\n",
    "3. M√©todo predict_proba(X) (Predicci√≥n):\n",
    "- Calcular Verosimilitud: Para una nueva muestra de prueba, usa los modelos KDE entrenados para calcular la verosimilitud (probabilidad de las caracter√≠sticas dadas las clases), $P(x_i \\mid y)$.\n",
    "- Aplicar la Regla de Bayes: Combina la Probabilidad a Priori ($P(y)$) con la Verosimilitud para obtener la probabilidad final de pertenencia a la clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6fbd521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase CustomNaiveBayesKDE definida exitosamente con suavizado (clipping).\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# PASO 5: IMPLEMENTACI√ìN DEL CLASIFICADOR NAIVE BAYES PERSONALIZADO\n",
    "# (INCLUYE CORRECCI√ìN DE SUAVIZADO/CLIPPING PARA EVITAR ERRORES NaN EN AUC ROC)\n",
    "# =================================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "# Definimos el valor de suavizado para el clipping\n",
    "SMOOTHING_CLIP = 1e-10\n",
    "\n",
    "class CustomNaiveBayesKDE(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, kernel='gaussian', bandwidth=1.0):\n",
    "        # Almacena el tipo de kernel y el bandwidth (h)\n",
    "        self.kernel = kernel\n",
    "        self.bandwidth = bandwidth\n",
    "        \n",
    "        # Inicializa las variables que se almacenar√°n durante el entrenamiento\n",
    "        self.classes_ = None\n",
    "        self.class_log_prior_ = None\n",
    "        self.feature_estimators_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        # Inicializar estructuras para almacenar las probabilidades a priori y los estimadores de densidad\n",
    "        self.class_log_prior_ = {}\n",
    "        self.feature_estimators_ = {}\n",
    "        \n",
    "        # 1. Calcular Probabilidad a Priori (P(y))\n",
    "        for c in self.classes_:\n",
    "            # Obtener √≠ndices de las muestras que pertenecen a la clase actual\n",
    "            X_c = X[y == c]\n",
    "            \n",
    "            # Calcular log(P(y))\n",
    "            self.class_log_prior_[c] = np.log(X_c.shape[0] / X.shape[0])\n",
    "            \n",
    "            # 2. Entrenar KDE para cada caracter√≠stica (P(x_i | y))\n",
    "            self.feature_estimators_[c] = {}\n",
    "            for i in range(n_features):\n",
    "                # Extraer la caracter√≠stica i para la clase c\n",
    "                feature_data = X_c.iloc[:, i].values.reshape(-1, 1)\n",
    "                \n",
    "                # Instanciar y entrenar el KDE con el kernel y bandwidth definidos\n",
    "                kde = KernelDensity(kernel=self.kernel, bandwidth=self.bandwidth)\n",
    "                kde.fit(feature_data)\n",
    "                self.feature_estimators_[c][i] = kde\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        # Inicializar matriz de log-probabilidades (una fila por muestra, una columna por clase)\n",
    "        log_prob_matrix = np.zeros((n_samples, len(self.classes_)))\n",
    "        \n",
    "        # Calcular log(P(y | x)) para cada clase\n",
    "        for idx, c in enumerate(self.classes_):\n",
    "            # Sumar el log-prior (log(P(y)))\n",
    "            log_prob_matrix[:, idx] = self.class_log_prior_[c]\n",
    "            \n",
    "            # Sumar las log-verosimilitudes (log(P(x_i | y))) para cada caracter√≠stica\n",
    "            n_features = X.shape[1]\n",
    "            for i in range(n_features):\n",
    "                feature_data = X.iloc[:, i].values.reshape(-1, 1)\n",
    "                \n",
    "                # Obtener log-verosimilitud del KDE\n",
    "                log_likelihoods = self.feature_estimators_[c][i].score_samples(feature_data)\n",
    "                \n",
    "                # log(P(y | x)) = log(P(y)) + SUM(log(P(x_i | y)))\n",
    "                log_prob_matrix[:, idx] += log_likelihoods\n",
    "\n",
    "        # 3. Convertir log-probabilidades a probabilidades normales\n",
    "        max_log_prob = np.max(log_prob_matrix, axis=1, keepdims=True)\n",
    "        exp_prob = np.exp(log_prob_matrix - max_log_prob)\n",
    "        \n",
    "        # Normalizar para que las probabilidades sumen 1\n",
    "        prob_matrix = exp_prob / np.sum(exp_prob, axis=1, keepdims=True)\n",
    "        \n",
    "        # üöÄ APLICAR CLIPPING: Asegurar que las probabilidades no sean 0 o 1 exacto\n",
    "        prob_matrix = np.clip(prob_matrix, SMOOTHING_CLIP, 1 - SMOOTHING_CLIP)\n",
    "        \n",
    "        return prob_matrix\n",
    "\n",
    "    def predict(self, X):\n",
    "        # La predicci√≥n es simplemente la clase con la probabilidad m√°s alta\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return self.classes_[np.argmax(probabilities, axis=1)]\n",
    "\n",
    "\n",
    "print(\"Clase CustomNaiveBayesKDE definida exitosamente con suavizado (clipping).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8e97bf",
   "metadata": {},
   "source": [
    "Paso 6: KDE con Regla de Silverman (M√©todo 2.c)\n",
    "\n",
    "Dado que la librer√≠a scipy.stats.gaussian_kde maneja el c√°lculo de la densidad de manera diferente a sklearn.neighbors.KernelDensity, crearemos una funci√≥n de entrenamiento separada.\n",
    "\n",
    "1. Importar Herramienta: Importar gaussian_kde de SciPy, que se basa en la Regla de Silverman.\n",
    "2. Definir CustomNaiveBayesKDE_Silverman: Creamos una nueva clase que hereda la l√≥gica general del Naive Bayes implementado en el Paso 5, pero que sobrescribe el m√©todo de entrenamiento (fit) para usar la funci√≥n de SciPy.\n",
    "3. Entrenamiento con Silverman: Dentro del fit, usamos gaussian_kde(feature_data) para entrenar la estimaci√≥n de densidad para cada caracter√≠stica y clase. Esta funci√≥n calcula internamente el $h$ √≥ptimo con la Regla de Silverman.\n",
    "4. C√°lculo de Verosimilitud: Para la predicci√≥n, usamos el m√©todo .logpdf() del objeto de SciPy para obtener el $\\log P(x_i \\mid y)$.\n",
    "5. Evaluaci√≥n: Entrenar y evaluar esta nueva clase para obtener el AUC ROC y compararlo con el Baseline (Paso 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad84e06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluaci√≥n de KDE con Regla de Silverman ---\n",
      "AUC ROC (KDE Silverman): 0.9134\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# PASO 6 (CORREGIDO): IMPLEMENTACI√ìN Y EVALUACI√ìN DE KDE CON REGLA DE SILVERMAN\n",
    "# Soluci√≥n al TypeError forzando la conversi√≥n a float.\n",
    "# =================================================================\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin # Importar para que la herencia funcione\n",
    "# Asume que CustomNaiveBayesKDE ya est√° definida del Paso 5\n",
    "\n",
    "class CustomNaiveBayesKDE_Silverman(CustomNaiveBayesKDE):\n",
    "    \n",
    "    # 6.1 Implementaci√≥n de FIT usando la Regla de Silverman (SciPy)\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_features = X.shape[1]\n",
    "        self.class_log_prior_ = {}\n",
    "        self.feature_estimators_ = {}\n",
    "        \n",
    "        for c in self.classes_:\n",
    "            X_c = X[y == c]\n",
    "            \n",
    "            # C√°lculo del log(P(y)) - Probabilidad a Priori\n",
    "            self.class_log_prior_[c] = np.log(X_c.shape[0] / X.shape[0])\n",
    "            self.feature_estimators_[c] = {}\n",
    "            \n",
    "            # Entrenamiento de KDE para cada caracter√≠stica usando SciPy\n",
    "            for i in range(n_features):\n",
    "                # CORRECCI√ìN CLAVE 1: Forzar la conversi√≥n a numpy array de flotantes\n",
    "                feature_data = X_c.iloc[:, i].values.astype(np.float64)\n",
    "                \n",
    "                # gaussian_kde calcula el bandwidth h autom√°ticamente con la Regla de Silverman\n",
    "                # Se requiere que feature_data sea 1D para SciPy\n",
    "                kde = gaussian_kde(feature_data) \n",
    "                self.feature_estimators_[c][i] = kde\n",
    "        \n",
    "        return self\n",
    "\n",
    "    # 6.2 Implementaci√≥n de PREDICT_PROBA adaptada a SciPy\n",
    "    def predict_proba(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        log_prob_matrix = np.zeros((n_samples, len(self.classes_)))\n",
    "        \n",
    "        for idx, c in enumerate(self.classes_):\n",
    "            # Sumar el log-prior (log(P(y)))\n",
    "            log_prob_matrix[:, idx] = self.class_log_prior_[c]\n",
    "            \n",
    "            n_features = X.shape[1]\n",
    "            for i in range(n_features):\n",
    "                # CORRECCI√ìN CLAVE 2: Forzar la conversi√≥n a numpy array de flotantes\n",
    "                feature_data = X.iloc[:, i].values.astype(np.float64)\n",
    "\n",
    "                # Usar logpdf() de SciPy para obtener la log-verosimilitud\n",
    "                log_likelihoods = self.feature_estimators_[c][i].logpdf(feature_data)\n",
    "                \n",
    "                log_prob_matrix[:, idx] += log_likelihoods\n",
    "\n",
    "        # Conversi√≥n de log-probabilidades a probabilidades (c√≥digo est√°ndar del Paso 5)\n",
    "        max_log_prob = np.max(log_prob_matrix, axis=1, keepdims=True)\n",
    "        exp_prob = np.exp(log_prob_matrix - max_log_prob)\n",
    "        prob_matrix = exp_prob / np.sum(exp_prob, axis=1, keepdims=True)\n",
    "        \n",
    "        return prob_matrix\n",
    "\n",
    "# 6.3 Evaluaci√≥n del Modelo con Regla de Silverman\n",
    "modelo_silverman = CustomNaiveBayesKDE_Silverman()\n",
    "# Asume que X_train y y_train est√°n listos del Paso 3\n",
    "modelo_silverman.fit(X_train, y_train) \n",
    "\n",
    "# Asume que X_test y y_test est√°n listos del Paso 3\n",
    "y_pred_proba_silverman = modelo_silverman.predict_proba(X_test)[:, 1]\n",
    "auc_roc_silverman = roc_auc_score(y_test, y_pred_proba_silverman)\n",
    "\n",
    "print(\"--- Evaluaci√≥n de KDE con Regla de Silverman ---\")\n",
    "print(f\"AUC ROC (KDE Silverman): {auc_roc_silverman:.4f}\")\n",
    "\n",
    "# Almacenar el resultado (asume que 'resultados' existe del Paso 4)\n",
    "# Aseg√∫rate de que la variable resultados existe y tiene el GNB baseline\n",
    "# resultados['KDE_Silverman'] = auc_roc_silverman\n",
    "# print(\"\\nResultados actuales para comparaci√≥n:\")\n",
    "# print(pd.Series(resultados))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eb9073",
   "metadata": {},
   "source": [
    "Paso 7: KDE Tipo Parzen (Ventanas Simples)\n",
    "Este paso implementar√° los m√©todos de KDE Tipo Parzen (ventanas simples: rectangular/tophat y triangular/linear) con un bandwidth $h$ fijo, tal como lo pide la Metodolog√≠a (Punto 2.b).\n",
    "Utilizaremos la clase base CustomNaiveBayesKDE que creamos en el Paso 5, ya que esa clase est√° dise√±ada para usar sklearn.neighbors.KernelDensity, la cual soporta estos kernels simples y requiere un $h$ fijo.\n",
    "1. Modelo Parzen (Tophat): Instanciar CustomNaiveBayesKDE con kernel='tophat' y un bandwidth $h$ fijo (ej., h=0.5).\n",
    "2. Entrenar y Evaluar (Tophat): Entrenar el modelo con $X_{train}$ y calcular su AUC ROC en $X_{test}$.\n",
    "3. Modelo Parzen (Linear): Instanciar el modelo con kernel='linear' y el mismo bandwidth $h$ fijo (ej., h=0.5).\n",
    "4. Entrenar y Evaluar (Linear): Entrenar y calcular su AUC ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71773fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando bandwidth fijo (h) = 0.5 para m√©todos Parzen.\n",
      "\n",
      "--- 7.1 Evaluaci√≥n KDE Parzen (Tophat) ---\n",
      "AUC ROC (KDE Tophat, h=0.5): 0.9115\n",
      "\n",
      "--- 7.2 Evaluaci√≥n KDE Parzen (Linear) ---\n",
      "AUC ROC (KDE Linear, h=0.5): 0.9156\n",
      "\n",
      "--- Resumen de Resultados Acumulados ---\n",
      "KDE_Parzen_Linear           0.915594\n",
      "KDE_Gaussiano_Optimizado    0.913236\n",
      "KDE_Parzen_Tophat           0.911450\n",
      "GaussianNB_Baseline         0.862633\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# Definimos un bandwidth fijo para el m√©todo Parzen (este valor es arbitrario para empezar)\n",
    "H_PARZEN_FIJO = 0.5 \n",
    "print(f\"Usando bandwidth fijo (h) = {H_PARZEN_FIJO} para m√©todos Parzen.\")\n",
    "\n",
    "# --- 7.1 Modelo KDE Parzen con Kernel Rectangular (Tophat) ---\n",
    "print(\"\\n--- 7.1 Evaluaci√≥n KDE Parzen (Tophat) ---\")\n",
    "modelo_parzen_tophat = CustomNaiveBayesKDE(kernel='tophat', bandwidth=H_PARZEN_FIJO)\n",
    "modelo_parzen_tophat.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba_tophat = modelo_parzen_tophat.predict_proba(X_test)[:, 1]\n",
    "auc_roc_tophat = roc_auc_score(y_test, y_pred_proba_tophat)\n",
    "\n",
    "print(f\"AUC ROC (KDE Tophat, h={H_PARZEN_FIJO}): {auc_roc_tophat:.4f}\")\n",
    "resultados['KDE_Parzen_Tophat'] = auc_roc_tophat\n",
    "\n",
    "\n",
    "# --- 7.2 Modelo KDE Parzen con Kernel Triangular (Linear) ---\n",
    "print(\"\\n--- 7.2 Evaluaci√≥n KDE Parzen (Linear) ---\")\n",
    "modelo_parzen_linear = CustomNaiveBayesKDE(kernel='linear', bandwidth=H_PARZEN_FIJO)\n",
    "modelo_parzen_linear.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba_linear = modelo_parzen_linear.predict_proba(X_test)[:, 1]\n",
    "auc_roc_linear = roc_auc_score(y_test, y_pred_proba_linear)\n",
    "\n",
    "print(f\"AUC ROC (KDE Linear, h={H_PARZEN_FIJO}): {auc_roc_linear:.4f}\")\n",
    "resultados['KDE_Parzen_Linear'] = auc_roc_linear\n",
    "\n",
    "\n",
    "# --- 7.3 Resumen de Resultados ---\n",
    "print(\"\\n--- Resumen de Resultados Acumulados ---\")\n",
    "print(pd.Series(resultados).sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e4feeb",
   "metadata": {},
   "source": [
    "8: KDE Gaussiano con Optimizaci√≥n Bayesiana\n",
    "\n",
    "Este paso implementa el KDE Gaussiano (M√©todo 2.a), pero en lugar de usar un bandwidth ($h$) fijo, utilizaremos una t√©cnica de b√∫squeda avanzada (GridSearchCV de Scikit-learn) para encontrar el mejor $h$ que maximice el rendimiento (AUC ROC).\n",
    "\n",
    "1. Definir la B√∫squeda: Definir el rango de valores (el grid) para el bandwidth $h$ que queremos probar (ej., $0.05$ a $2.0$).\n",
    "2. Configurar GridSearchCV: Utilizar GridSearchCV junto con tu clase CustomNaiveBayesKDE para buscar el mejor $h$. GridSearchCV autom√°ticamente entrena y eval√∫a tu clasificador para cada valor de $h$ en el grid\n",
    "3. M√©trica de B√∫squeda: Especificar que la m√©trica a optimizar debe ser 'roc_auc' (AUC ROC).\n",
    "4. Entrenar y Encontrar $h$: Entrenar el GridSearchCV usando los datos de entrenamiento. Esto es el proceso de optimizaci√≥n.\n",
    "5. Evaluar el Mejor Modelo: Tomar el mejor modelo encontrado (el que usa el $h$ √≥ptimo) y evaluarlo en el conjunto de prueba (X_test) final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "634f0fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 8.1 Optimizando el Bandwidth (h) para KDE Gaussiano ---\n",
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kenet\\Documents\\keneth_bracamonte_clase_IA\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 8.6 Evaluaci√≥n Final ---\n",
      "El mejor bandwidth (h) encontrado es: 0.1000\n",
      "AUC ROC (KDE Gaussiano Optimizado): 0.9132\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"--- 8.1 Optimizando el Bandwidth (h) para KDE Gaussiano ---\")\n",
    "\n",
    "# 8.2 Definir el rango de valores de 'h' (bandwidth) a probar\n",
    "# Es crucial probar un rango apropiado. Usamos una distribuci√≥n logar√≠tmica.\n",
    "# Esto toma tiempo, por eso el rango es limitado.\n",
    "param_grid = {\n",
    "    'bandwidth': np.logspace(-1, 0.3, 15) # Rango de h desde ~0.1 hasta ~2.0\n",
    "}\n",
    "\n",
    "# 8.3 Configurar GridSearchCV para buscar el mejor 'h'\n",
    "# Instanciamos tu clasificador personalizado con el kernel Gaussiano\n",
    "nb_kde_gauss = CustomNaiveBayesKDE(kernel='gaussian')\n",
    "\n",
    "# Instanciar el GridSearchCV\n",
    "# scoring='roc_auc' le dice que optimice usando el AUC ROC\n",
    "# cv=5 usa 5-fold Cross-Validation en el conjunto de entrenamiento\n",
    "grid_search = GridSearchCV(\n",
    "    nb_kde_gauss,\n",
    "    param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=5, \n",
    "    verbose=1,\n",
    "    n_jobs=-1 # Usar todos los n√∫cleos del procesador\n",
    ")\n",
    "\n",
    "# 8.4 Entrenar y encontrar el h √≥ptimo\n",
    "# Esto toma tiempo ya que entrena 15 valores * 5 folds = 75 modelos.\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 8.5 Obtener el mejor resultado y evaluarlo en X_test\n",
    "best_h = grid_search.best_params_['bandwidth']\n",
    "best_model_opt = grid_search.best_estimator_\n",
    "\n",
    "# Predecir las probabilidades con el modelo √≥ptimo\n",
    "y_pred_proba_opt = best_model_opt.predict_proba(X_test)[:, 1]\n",
    "auc_roc_opt = roc_auc_score(y_test, y_pred_proba_opt)\n",
    "\n",
    "print(f\"\\n--- 8.6 Evaluaci√≥n Final ---\")\n",
    "print(f\"El mejor bandwidth (h) encontrado es: {best_h:.4f}\")\n",
    "print(f\"AUC ROC (KDE Gaussiano Optimizado): {auc_roc_opt:.4f}\")\n",
    "\n",
    "# Almacenar el resultado final\n",
    "resultados['KDE_Gaussiano_Optimizado'] = auc_roc_opt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8d4df6",
   "metadata": {},
   "source": [
    "Paso 9: Imprimir resultado\n",
    "\n",
    "En este paso se imprimen los resultado de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4de3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PASO 9: RESUMEN FINAL DE LA METODOLOG√çA ---\n",
      "\n",
      "Resultados de los 4 Modelos de Naive Bayes:\n",
      "                           AUC_ROC\n",
      "KDE_Parzen_Linear         0.915594\n",
      "KDE_Gaussiano_Optimizado  0.913236\n",
      "KDE_Parzen_Tophat         0.911450\n",
      "GaussianNB_Baseline       0.862633\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- PASO 9: RESUMEN FINAL DE LA METODOLOG√çA ---\")\n",
    "resultados_df = pd.Series(resultados).sort_values(ascending=False).to_frame(name='AUC_ROC')\n",
    "\n",
    "print(\"\\nResultados de los 4 Modelos de Naive Bayes:\")\n",
    "print(resultados_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
