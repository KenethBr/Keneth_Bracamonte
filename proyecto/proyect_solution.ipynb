{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4eae556",
   "metadata": {},
   "source": [
    "Paso 1: Carga del Dataset\n",
    "1. Importar Pandas y NumPy: Son las librerías fundamentales para manejar datos (pandas) y realizar cálculos numéricos eficientes (numpy).\n",
    "\n",
    "2. Cargar el Archivo: Usamos pd.read_csv() para leer el archivo del AI4I 2020 Predictive Maintenance Dataset. Asumiremos que el archivo se llama ai4i2020.csv y se encuentra en la misma carpeta que tu Jupyter Notebook.\n",
    "\n",
    "3. Mostrar Encabezado: Usamos .head() para ver las primeras 10 filas y confirmar que la carga fue exitosa y para empezar a inspeccionar el formato de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77ab87ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UDI</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>Air temperature [K]</th>\n",
       "      <th>Process temperature [K]</th>\n",
       "      <th>Rotational speed [rpm]</th>\n",
       "      <th>Torque [Nm]</th>\n",
       "      <th>Tool wear [min]</th>\n",
       "      <th>Machine failure</th>\n",
       "      <th>TWF</th>\n",
       "      <th>HDF</th>\n",
       "      <th>PWF</th>\n",
       "      <th>OSF</th>\n",
       "      <th>RNF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>M14860</td>\n",
       "      <td>M</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1551</td>\n",
       "      <td>42.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>L47181</td>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408</td>\n",
       "      <td>46.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>L47182</td>\n",
       "      <td>L</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.5</td>\n",
       "      <td>1498</td>\n",
       "      <td>49.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>L47183</td>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1433</td>\n",
       "      <td>39.5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>L47184</td>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408</td>\n",
       "      <td>40.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>M14865</td>\n",
       "      <td>M</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1425</td>\n",
       "      <td>41.9</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>L47186</td>\n",
       "      <td>L</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1558</td>\n",
       "      <td>42.4</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>L47187</td>\n",
       "      <td>L</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1527</td>\n",
       "      <td>40.2</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>M14868</td>\n",
       "      <td>M</td>\n",
       "      <td>298.3</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1667</td>\n",
       "      <td>28.6</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>M14869</td>\n",
       "      <td>M</td>\n",
       "      <td>298.5</td>\n",
       "      <td>309.0</td>\n",
       "      <td>1741</td>\n",
       "      <td>28.0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UDI     Product ID   Type   Air temperature [K]   Process temperature [K]  \\\n",
       "0      1   M14860       M                    298.1                     308.6   \n",
       "1      2   L47181       L                    298.2                     308.7   \n",
       "2      3   L47182       L                    298.1                     308.5   \n",
       "3      4   L47183       L                    298.2                     308.6   \n",
       "4      5   L47184       L                    298.2                     308.7   \n",
       "5      6   M14865       M                    298.1                     308.6   \n",
       "6      7   L47186       L                    298.1                     308.6   \n",
       "7      8   L47187       L                    298.1                     308.6   \n",
       "8      9   M14868       M                    298.3                     308.7   \n",
       "9     10   M14869       M                    298.5                     309.0   \n",
       "\n",
       "    Rotational speed [rpm]   Torque [Nm]   Tool wear [min]   Machine failure  \\\n",
       "0                     1551          42.8                 0                 0   \n",
       "1                     1408          46.3                 3                 0   \n",
       "2                     1498          49.4                 5                 0   \n",
       "3                     1433          39.5                 7                 0   \n",
       "4                     1408          40.0                 9                 0   \n",
       "5                     1425          41.9                11                 0   \n",
       "6                     1558          42.4                14                 0   \n",
       "7                     1527          40.2                16                 0   \n",
       "8                     1667          28.6                18                 0   \n",
       "9                     1741          28.0                21                 0   \n",
       "\n",
       "    TWF   HDF   PWF   OSF   RNF  \n",
       "0     0     0     0     0     0  \n",
       "1     0     0     0     0     0  \n",
       "2     0     0     0     0     0  \n",
       "3     0     0     0     0     0  \n",
       "4     0     0     0     0     0  \n",
       "5     0     0     0     0     0  \n",
       "6     0     0     0     0     0  \n",
       "7     0     0     0     0     0  \n",
       "8     0     0     0     0     0  \n",
       "9     0     0     0     0     0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"./ai4i2020.csv\")\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9060bc8",
   "metadata": {},
   "source": [
    "Paso 2: Limpieza de nombres de columnas y Análisis Exploratorio de Datos (EDA)\n",
    "1. Crea una herramienta que toma un nombre de columna, elimina espacios al inicio/final, y reemplaza secuencias de múltiples espacios con un solo espacio.\n",
    "2. df.info(): Imprime un resumen de cuántos valores no nulos hay en cada columna y el tipo de dato (Dtype).\n",
    "3. df.describe(): Calcula estadísticas básicas (media, min, max, desviación estándar) para las columnas numéricas.\n",
    "4. df[NOMBRE_OBJETIVO].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b3712ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2.2.1 Información General del DataFrame ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   UDI                      10000 non-null  int64  \n",
      " 1   Product ID               10000 non-null  object \n",
      " 2   Type                     10000 non-null  object \n",
      " 3   Air temperature [K]      10000 non-null  float64\n",
      " 4   Process temperature [K]  10000 non-null  float64\n",
      " 5   Rotational speed [rpm]   10000 non-null  int64  \n",
      " 6   Torque [Nm]              10000 non-null  float64\n",
      " 7   Tool wear [min]          10000 non-null  int64  \n",
      " 8   Machine failure          10000 non-null  int64  \n",
      " 9   TWF                      10000 non-null  int64  \n",
      " 10  HDF                      10000 non-null  int64  \n",
      " 11  PWF                      10000 non-null  int64  \n",
      " 12  OSF                      10000 non-null  int64  \n",
      " 13  RNF                      10000 non-null  int64  \n",
      "dtypes: float64(3), int64(9), object(2)\n",
      "memory usage: 1.1+ MB\n",
      "\n",
      "--- 2.2.2 Estadísticas Descriptivas de Variables Numéricas ---\n",
      "               UDI  Air temperature [K]  Process temperature [K]  \\\n",
      "count  10000.00000         10000.000000             10000.000000   \n",
      "mean    5000.50000           300.004930               310.005560   \n",
      "std     2886.89568             2.000259                 1.483734   \n",
      "min        1.00000           295.300000               305.700000   \n",
      "25%     2500.75000           298.300000               308.800000   \n",
      "50%     5000.50000           300.100000               310.100000   \n",
      "75%     7500.25000           301.500000               311.100000   \n",
      "max    10000.00000           304.500000               313.800000   \n",
      "\n",
      "       Rotational speed [rpm]   Torque [Nm]  Tool wear [min]  Machine failure  \\\n",
      "count            10000.000000  10000.000000     10000.000000     10000.000000   \n",
      "mean              1538.776100     39.986910       107.951000         0.033900   \n",
      "std                179.284096      9.968934        63.654147         0.180981   \n",
      "min               1168.000000      3.800000         0.000000         0.000000   \n",
      "25%               1423.000000     33.200000        53.000000         0.000000   \n",
      "50%               1503.000000     40.100000       108.000000         0.000000   \n",
      "75%               1612.000000     46.800000       162.000000         0.000000   \n",
      "max               2886.000000     76.600000       253.000000         1.000000   \n",
      "\n",
      "                TWF           HDF           PWF           OSF          RNF  \n",
      "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.00000  \n",
      "mean       0.004600      0.011500      0.009500      0.009800      0.00190  \n",
      "std        0.067671      0.106625      0.097009      0.098514      0.04355  \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.00000  \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.00000  \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.00000  \n",
      "75%        0.000000      0.000000      0.000000      0.000000      0.00000  \n",
      "max        1.000000      1.000000      1.000000      1.000000      1.00000  \n",
      "\n",
      "--- 2.3.1 Conteo de la Variable Objetivo ('Machine failure') ---\n",
      "Machine failure\n",
      "0    9661\n",
      "1     339\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Porcentaje de Fallo de Máquina (Clase 1): 3.39%\n",
      "Este bajo porcentaje confirma un problema de desbalance de clases.\n"
     ]
    }
   ],
   "source": [
    "# Función para limpiar nombres: reemplaza múltiples espacios por uno y elimina espacios al inicio/final\n",
    "def limpiar_nombre_columna(col):\n",
    "    # ' '.join(col.split()) elimina cualquier secuencia de espacios y la reemplaza por un solo espacio.\n",
    "    col = ' '.join(col.split())\n",
    "    return col.strip()\n",
    "\n",
    "# Aplicar la limpieza a todos los nombres de columna\n",
    "df.columns = [limpiar_nombre_columna(col) for col in df.columns]\n",
    "\"\"\"\"\n",
    "print(\"--- Nombres de columnas después de la limpieza ---\")\n",
    "print(df.columns.tolist())\n",
    "\"\"\"\n",
    "\n",
    "# --- 2.2 ANÁLISIS DE TIPOS DE DATOS Y ESTADÍSTICAS ---\n",
    "\n",
    "print(\"\\n--- 2.2.1 Información General del DataFrame ---\")\n",
    "# Muestra el tipo de dato y la cuenta de valores no nulos (verificar si hay NaNs)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n--- 2.2.2 Estadísticas Descriptivas de Variables Numéricas ---\")\n",
    "# Muestra estadísticas como media, desviación estándar, min/max.\n",
    "print(df.describe())\n",
    "\n",
    "\n",
    "# --- 2.3 ANÁLISIS DE LA VARIABLE OBJETIVO ---\n",
    "\n",
    "print(f\"\\n--- 2.3.1 Conteo de la Variable Objetivo ('Machine failure') ---\")\n",
    "# Analiza el balance de la variable objetivo (0 = No Fallo, 1 = Fallo)\n",
    "conteo_fallos = df['Machine failure'].value_counts()\n",
    "print(conteo_fallos)\n",
    "\n",
    "# Calcular el porcentaje de fallos\n",
    "porcentaje_fallos = (conteo_fallos.get(1, 0) / df.shape[0]) * 100\n",
    "print(f\"\\nPorcentaje de Fallo de Máquina (Clase 1): {porcentaje_fallos:.2f}%\")\n",
    "print(\"Este bajo porcentaje confirma un problema de desbalance de clases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f293b0",
   "metadata": {},
   "source": [
    "Paso 3: Preprocesamiento de Datos y Escalado\n",
    "\n",
    "1. Eliminar Columnas Innecesarias: Se eliminan las columnas de identificación (UDI, Product ID) que no tienen poder predictivo.\n",
    "\n",
    "2. Separar X e y: El dataset se divide en características predictoras (X) y la variable objetivo (y), que es 'Machine failure'.\n",
    "\n",
    "3. Codificación de Variable Categórica: Se aplica One-Hot Encoding a la columna 'Type' (L, M, H) para convertirla en columnas binarias (ej: Type_L, Type_M), ya que los modelos matemáticos solo trabajan con números.\n",
    "\n",
    "4. Identificar Numéricas: Se identifican las columnas con valores continuos (temperatura, velocidad, torque, etc.).\n",
    "\n",
    "5. Escalado (StandardScaler): Se aplica StandardScaler a las columnas numéricas. Esto es crucial: centra los datos en una media de 0 y desviación estándar de 1, igualando las escalas para que Naive Bayes y KDE funcionen correctamente.\n",
    "\n",
    "6. División de Datos (70/30): El dataset se divide en un 70% para entrenamiento (X_train, y_train) y un 30% para prueba (X_test, y_test). La opción stratify=y asegura que la proporción de fallos (el desbalance) se mantenga igual en ambos conjuntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "640797b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3.3 Codificación de Variable Categórica ('Type') ---\n",
      "Columnas después de codificación: ['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]', 'TWF', 'HDF', 'PWF', 'OSF', 'RNF', 'Type_ H   ', 'Type_ L   ', 'Type_ M   ']\n",
      "\n",
      "Primeras filas de X después del escalado:\n",
      "   Air temperature [K]  Process temperature [K]  Rotational speed [rpm]  \\\n",
      "0            -0.952389                -0.947360                0.068185   \n",
      "1            -0.902393                -0.879959               -0.729472   \n",
      "2            -0.952389                -1.014761               -0.227450   \n",
      "3            -0.902393                -0.947360               -0.590021   \n",
      "4            -0.902393                -0.879959               -0.729472   \n",
      "\n",
      "   Torque [Nm]  Tool wear [min]      TWF      HDF       PWF       OSF  \\\n",
      "0     0.282200        -1.695984 -0.06798 -0.10786 -0.097934 -0.099484   \n",
      "1     0.633308        -1.648852 -0.06798 -0.10786 -0.097934 -0.099484   \n",
      "2     0.944290        -1.617430 -0.06798 -0.10786 -0.097934 -0.099484   \n",
      "3    -0.048845        -1.586009 -0.06798 -0.10786 -0.097934 -0.099484   \n",
      "4     0.001313        -1.554588 -0.06798 -0.10786 -0.097934 -0.099484   \n",
      "\n",
      "       RNF  Type_ H     Type_ L     Type_ M     \n",
      "0 -0.04363       False       False        True  \n",
      "1 -0.04363       False        True       False  \n",
      "2 -0.04363       False        True       False  \n",
      "3 -0.04363       False        True       False  \n",
      "4 -0.04363       False        True       False  \n",
      "\n",
      "--- 3.7 Resumen de la División ---\n",
      "Tamaño de Entrenamiento (X_train): (7000, 13)\n",
      "Tamaño de Prueba (X_test): (3000, 13)\n"
     ]
    }
   ],
   "source": [
    "# Importar librerías necesarias para preprocesamiento\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 3.1 Eliminar Columnas Innecesarias\n",
    "# Eliminamos los identificadores UDI y Product ID\n",
    "df = df.drop(columns=['UDI', 'Product ID'])\n",
    "\n",
    "# 3.2 Separar X (predictoras) e y (objetivo)\n",
    "NOMBRE_OBJETIVO = 'Machine failure'\n",
    "X = df.drop(columns=[NOMBRE_OBJETIVO])\n",
    "y = df[NOMBRE_OBJETIVO]\n",
    "\n",
    "print(\"--- 3.3 Codificación de Variable Categórica ('Type') ---\")\n",
    "# Aplicar One-Hot Encoding a la columna 'Type' (L, M, H)\n",
    "X = pd.get_dummies(X, columns=['Type'], drop_first=False)\n",
    "print(f\"Columnas después de codificación: {X.columns.tolist()}\")\n",
    "\n",
    "# 3.4 Identificar columnas numéricas para escalado\n",
    "# Todas las columnas excepto las binarias (Type_L, Type_M, Type_H, y fallos específicos) son numéricas\n",
    "# Usamos select_dtypes para mayor seguridad\n",
    "columnas_numericas = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# 3.5 Instanciar y aplicar el escalador (StandardScaler)\n",
    "# Esto centra los datos en media 0 y desviación estándar 1. Vital para KDE.\n",
    "scaler = StandardScaler()\n",
    "X[columnas_numericas] = scaler.fit_transform(X[columnas_numericas])\n",
    "\n",
    "print(\"\\nPrimeras filas de X después del escalado:\")\n",
    "print(X.head())\n",
    "\n",
    "# 3.6 División en conjuntos de entrenamiento (70%) y prueba (30%)\n",
    "# stratify=y asegura que la proporción de fallos se mantenga en ambos conjuntos\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\n--- 3.7 Resumen de la División ---\")\n",
    "print(f\"Tamaño de Entrenamiento (X_train): {X_train.shape}\")\n",
    "print(f\"Tamaño de Prueba (X_test): {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a1dee4",
   "metadata": {},
   "source": [
    "Paso 4: Establecer la Línea Base (IA 1 - GaussianNB)\n",
    "\n",
    "Este paso implementa tu primera IA (IA 1), que es el modelo Gaussian Naive Bayes estándar de Scikit-learn. Este modelo actúa como el punto de referencia (baseline) con el cual compararás tus implementaciones de Naive Bayes con KDE más adelante.\n",
    "\n",
    "1. Importar Herramientas: Importar las clases necesarias: GaussianNB (el modelo) y roc_auc_score (la métrica requerida en tu metodología).\n",
    "\n",
    "2. Instanciar y Entrenar: Se inicializa el modelo GaussianNB y se entrena usando el conjunto de entrenamiento (X_train, y_train) que preparamos en el Paso 3.\n",
    "\n",
    "3. Predicción de Probabilidades: Se utiliza el modelo para predecir las probabilidades de pertenencia a la clase (0 o 1) en el conjunto de prueba (X_test). La métrica AUC ROC requiere probabilidades, no solo la clase final.\n",
    "\n",
    "4. Evaluación: Se calcula el valor AUC ROC usando las probabilidades predichas y los valores reales (y_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de64cbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluación de la Línea Base (Gaussian Naive Bayes) ---\n",
      "AUC ROC del Baseline (GaussianNB): 0.9931\n"
     ]
    }
   ],
   "source": [
    "# 4.1 Importar el modelo y la métrica de evaluación\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# 4.2 Instanciar y Entrenar el modelo\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X_train, y_train)\n",
    "\n",
    "# 4.3 Predecir las probabilidades en el conjunto de prueba\n",
    "# Usamos predict_proba para obtener la probabilidad de que sea la clase 1 (fallo)\n",
    "y_pred_proba_gnb = gnb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 4.4 Evaluar el modelo con la métrica AUC ROC\n",
    "auc_roc_gnb = roc_auc_score(y_test, y_pred_proba_gnb)\n",
    "\n",
    "print(\"--- Evaluación de la Línea Base (Gaussian Naive Bayes) ---\")\n",
    "print(f\"AUC ROC del Baseline (GaussianNB): {auc_roc_gnb:.4f}\")\n",
    "\n",
    "# Guardamos el resultado del baseline para compararlo en pasos futuros\n",
    "resultados = {'GaussianNB_Baseline': auc_roc_gnb}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbf5ac2",
   "metadata": {},
   "source": [
    "Paso 5: Implementación del Clasificador Naive Bayes Personalizado\n",
    "\n",
    "Este código implementa la lógica de Naive Bayes utilizando el logaritmo de las probabilidades para evitar errores de underflow (probabilidades muy pequeñas). Usaremos KernelDensity de Scikit-learn como la herramienta para la estimación.\n",
    "\n",
    "1. Definir la Clase: Creamos una clase CustomNaiveBayesKDE con un método de inicialización (__init__) para almacenar el método KDE y el bandwidth $h$ que se usarán.\n",
    "2. Método fit(X, y) (Entrenamiento):\n",
    "- Calcular Probabilidad a Priori: Calcula $P(y)$, la probabilidad de cada clase (Fallo y No Fallo).\n",
    "- Entrenar KDE: Para cada característica $x_i$ y para cada clase $y$, ajusta el estimador de densidad (KDE) a los datos de entrenamiento. Estos modelos ajustados se almacenan.\n",
    "3. Método predict_proba(X) (Predicción):\n",
    "- Calcular Verosimilitud: Para una nueva muestra de prueba, usa los modelos KDE entrenados para calcular la verosimilitud (probabilidad de las características dadas las clases), $P(x_i \\mid y)$.\n",
    "- Aplicar la Regla de Bayes: Combina la Probabilidad a Priori ($P(y)$) con la Verosimilitud para obtener la probabilidad final de pertenencia a la clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b6fbd521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase CustomNaiveBayesKDE definida exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class CustomNaiveBayesKDE(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, kernel='gaussian', bandwidth=1.0):\n",
    "        # Almacena el tipo de kernel y el bandwidth (h)\n",
    "        self.kernel = kernel\n",
    "        self.bandwidth = bandwidth\n",
    "        \n",
    "        # Inicializa las variables que se almacenarán durante el entrenamiento\n",
    "        self.classes_ = None\n",
    "        self.class_log_prior_ = None\n",
    "        self.feature_estimators_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        # Inicializar estructuras para almacenar las probabilidades a priori y los estimadores de densidad\n",
    "        self.class_log_prior_ = {}\n",
    "        self.feature_estimators_ = {}\n",
    "        \n",
    "        # 1. Calcular Probabilidad a Priori (P(y))\n",
    "        for c in self.classes_:\n",
    "            # Obtener índices de las muestras que pertenecen a la clase actual\n",
    "            X_c = X[y == c]\n",
    "            \n",
    "            # Calcular log(P(y))\n",
    "            # Usamos logaritmos para evitar el underflow numérico\n",
    "            self.class_log_prior_[c] = np.log(X_c.shape[0] / X.shape[0])\n",
    "            \n",
    "            # 2. Entrenar KDE para cada característica (P(x_i | y))\n",
    "            self.feature_estimators_[c] = {}\n",
    "            for i in range(n_features):\n",
    "                # Extraer la característica i para la clase c\n",
    "                feature_data = X_c.iloc[:, i].values.reshape(-1, 1)\n",
    "                \n",
    "                # Instanciar y entrenar el KDE con el kernel y bandwidth definidos\n",
    "                kde = KernelDensity(kernel=self.kernel, bandwidth=self.bandwidth)\n",
    "                kde.fit(feature_data)\n",
    "                self.feature_estimators_[c][i] = kde\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        # Inicializar matriz de log-probabilidades (una fila por muestra, una columna por clase)\n",
    "        log_prob_matrix = np.zeros((n_samples, len(self.classes_)))\n",
    "        \n",
    "        # Calcular log(P(y | x)) para cada clase\n",
    "        for idx, c in enumerate(self.classes_):\n",
    "            # Sumar el log-prior (log(P(y)))\n",
    "            log_prob_matrix[:, idx] = self.class_log_prior_[c]\n",
    "            \n",
    "            # Sumar las log-verosimilitudes (log(P(x_i | y))) para cada característica\n",
    "            n_features = X.shape[1]\n",
    "            for i in range(n_features):\n",
    "                feature_data = X.iloc[:, i].values.reshape(-1, 1)\n",
    "                \n",
    "                # Obtener log-verosimilitud del KDE para la característica i y la clase c\n",
    "                log_likelihoods = self.feature_estimators_[c][i].score_samples(feature_data)\n",
    "                \n",
    "                # log(P(y | x)) = log(P(y)) + SUM(log(P(x_i | y)))\n",
    "                log_prob_matrix[:, idx] += log_likelihoods\n",
    "\n",
    "        # Convertir log-probabilidades a probabilidades normales (prob_matrix)\n",
    "        # Esto requiere normalizar las log-probabilidades para obtener probabilidades válidas\n",
    "        \n",
    "        # Restar el máximo para evitar sobreflujo al exponer\n",
    "        max_log_prob = np.max(log_prob_matrix, axis=1, keepdims=True)\n",
    "        exp_prob = np.exp(log_prob_matrix - max_log_prob)\n",
    "        \n",
    "        # Normalizar para que las probabilidades sumen 1\n",
    "        prob_matrix = exp_prob / np.sum(exp_prob, axis=1, keepdims=True)\n",
    "        \n",
    "        return prob_matrix\n",
    "\n",
    "    def predict(self, X):\n",
    "        # La predicción es simplemente la clase con la probabilidad más alta\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return self.classes_[np.argmax(probabilities, axis=1)]\n",
    "\n",
    "\n",
    "# Prueba rápida de la estructura (no es una prueba real de KDE aún)\n",
    "print(\"Clase CustomNaiveBayesKDE definida exitosamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8e97bf",
   "metadata": {},
   "source": [
    "Paso 6: KDE con Regla de Silverman (Método 2.c)\n",
    "\n",
    "Dado que la librería scipy.stats.gaussian_kde maneja el cálculo de la densidad de manera diferente a sklearn.neighbors.KernelDensity, crearemos una función de entrenamiento separada.\n",
    "\n",
    "1. Importar Herramienta: Importar gaussian_kde de SciPy, que se basa en la Regla de Silverman.\n",
    "2. Definir CustomNaiveBayesKDE_Silverman: Creamos una nueva clase que hereda la lógica general del Naive Bayes implementado en el Paso 5, pero que sobrescribe el método de entrenamiento (fit) para usar la función de SciPy.\n",
    "3. Entrenamiento con Silverman: Dentro del fit, usamos gaussian_kde(feature_data) para entrenar la estimación de densidad para cada característica y clase. Esta función calcula internamente el $h$ óptimo con la Regla de Silverman.\n",
    "4. Cálculo de Verosimilitud: Para la predicción, usamos el método .logpdf() del objeto de SciPy para obtener el $\\log P(x_i \\mid y)$.\n",
    "5. Evaluación: Entrenar y evaluar esta nueva clase para obtener el AUC ROC y compararlo con el Baseline (Paso 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ad84e06c",
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "The data appears to lie in a lower-dimensional subspace of the space in which it is expressed. This has resulted in a singular data covariance matrix, which cannot be treated using the algorithms implemented in `gaussian_kde`. Consider performing principal component analysis / dimensionality reduction and using `gaussian_kde` with the transformed data.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLinAlgError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kenet\\Documents\\keneth_bracamonte_clase_IA\\.venv\\Lib\\site-packages\\scipy\\stats\\_kde.py:235\u001b[39m, in \u001b[36mgaussian_kde.__init__\u001b[39m\u001b[34m(self, dataset, bw_method, weights)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mset_bandwidth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbw_method\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m linalg.LinAlgError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kenet\\Documents\\keneth_bracamonte_clase_IA\\.venv\\Lib\\site-packages\\scipy\\stats\\_kde.py:578\u001b[39m, in \u001b[36mgaussian_kde.set_bandwidth\u001b[39m\u001b[34m(self, bw_method)\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m578\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_covariance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kenet\\Documents\\keneth_bracamonte_clase_IA\\.venv\\Lib\\site-packages\\scipy\\stats\\_kde.py:590\u001b[39m, in \u001b[36mgaussian_kde._compute_covariance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    587\u001b[39m     \u001b[38;5;28mself\u001b[39m._data_covariance = atleast_2d(cov(\u001b[38;5;28mself\u001b[39m.dataset, rowvar=\u001b[32m1\u001b[39m,\n\u001b[32m    588\u001b[39m                                        bias=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    589\u001b[39m                                        aweights=\u001b[38;5;28mself\u001b[39m.weights))\n\u001b[32m--> \u001b[39m\u001b[32m590\u001b[39m     \u001b[38;5;28mself\u001b[39m._data_cho_cov = \u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_covariance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43mlower\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[38;5;28mself\u001b[39m.covariance = \u001b[38;5;28mself\u001b[39m._data_covariance * \u001b[38;5;28mself\u001b[39m.factor**\u001b[32m2\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kenet\\Documents\\keneth_bracamonte_clase_IA\\.venv\\Lib\\site-packages\\scipy\\_lib\\_util.py:1233\u001b[39m, in \u001b[36m_apply_over_batch.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(batch_shapes):\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mother_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[38;5;66;03m# Determine broadcasted batch shape\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kenet\\Documents\\keneth_bracamonte_clase_IA\\.venv\\Lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py:106\u001b[39m, in \u001b[36mcholesky\u001b[39m\u001b[34m(a, lower, overwrite_a, check_finite)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[33;03mCompute the Cholesky decomposition of a matrix.\u001b[39;00m\n\u001b[32m     54\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    104\u001b[39m \n\u001b[32m    105\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m c, lower = \u001b[43m_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_a\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m c\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kenet\\Documents\\keneth_bracamonte_clase_IA\\.venv\\Lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py:39\u001b[39m, in \u001b[36m_cholesky\u001b[39m\u001b[34m(a, lower, overwrite_a, clean, check_finite)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m info > \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\n\u001b[32m     40\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minfo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-th leading minor of the array is not positive definite\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     41\u001b[39m     )\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m info < \u001b[32m0\u001b[39m:\n",
      "\u001b[31mLinAlgError\u001b[39m: 1-th leading minor of the array is not positive definite",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mLinAlgError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# 6.3 Evaluación del Modelo con Regla de Silverman\u001b[39;00m\n\u001b[32m     64\u001b[39m modelo_silverman = CustomNaiveBayesKDE_Silverman()\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[43mmodelo_silverman\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m y_pred_proba_silverman = modelo_silverman.predict_proba(X_test)[:, \u001b[32m1\u001b[39m]\n\u001b[32m     68\u001b[39m auc_roc_silverman = roc_auc_score(y_test, y_pred_proba_silverman)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mCustomNaiveBayesKDE_Silverman.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m     27\u001b[39m         feature_data = X_c.iloc[:, i].values\n\u001b[32m     29\u001b[39m         \u001b[38;5;66;03m# gaussian_kde calcula el bandwidth h automáticamente con la Regla de Silverman\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m         kde = \u001b[43mgaussian_kde\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_data\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m     31\u001b[39m         \u001b[38;5;28mself\u001b[39m.feature_estimators_[c][i] = kde\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kenet\\Documents\\keneth_bracamonte_clase_IA\\.venv\\Lib\\site-packages\\scipy\\stats\\_kde.py:244\u001b[39m, in \u001b[36mgaussian_kde.__init__\u001b[39m\u001b[34m(self, dataset, bw_method, weights)\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m linalg.LinAlgError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    237\u001b[39m     msg = (\u001b[33m\"\u001b[39m\u001b[33mThe data appears to lie in a lower-dimensional subspace \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    238\u001b[39m            \u001b[33m\"\u001b[39m\u001b[33mof the space in which it is expressed. This has resulted \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    239\u001b[39m            \u001b[33m\"\u001b[39m\u001b[33min a singular data covariance matrix, which cannot be \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    242\u001b[39m            \u001b[33m\"\u001b[39m\u001b[33manalysis / dimensionality reduction and using \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m            \u001b[33m\"\u001b[39m\u001b[33m`gaussian_kde` with the transformed data.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m linalg.LinAlgError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mLinAlgError\u001b[39m: The data appears to lie in a lower-dimensional subspace of the space in which it is expressed. This has resulted in a singular data covariance matrix, which cannot be treated using the algorithms implemented in `gaussian_kde`. Consider performing principal component analysis / dimensionality reduction and using `gaussian_kde` with the transformed data."
     ]
    }
   ],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Creamos una clase que sobrescribe el método fit para usar gaussian_kde de SciPy\n",
    "# Herencia de la clase base de Scikit-learn para compatibilidad\n",
    "class CustomNaiveBayesKDE_Silverman(CustomNaiveBayesKDE):\n",
    "    \n",
    "    # 6.1 Implementación de FIT usando la Regla de Silverman (SciPy)\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_features = X.shape[1]\n",
    "        self.class_log_prior_ = {}\n",
    "        self.feature_estimators_ = {}\n",
    "        \n",
    "        for c in self.classes_:\n",
    "            X_c = X[y == c]\n",
    "            \n",
    "            # Cálculo del log(P(y)) - Probabilidad a Priori\n",
    "            self.class_log_prior_[c] = np.log(X_c.shape[0] / X.shape[0])\n",
    "            self.feature_estimators_[c] = {}\n",
    "            \n",
    "            # Entrenamiento de KDE para cada característica usando SciPy\n",
    "            for i in range(n_features):\n",
    "                # SciPy requiere que los datos sean una matriz 1D\n",
    "                feature_data = X_c.iloc[:, i].values\n",
    "                \n",
    "                # gaussian_kde calcula el bandwidth h automáticamente con la Regla de Silverman\n",
    "                kde = gaussian_kde(feature_data) \n",
    "                self.feature_estimators_[c][i] = kde\n",
    "        \n",
    "        return self\n",
    "\n",
    "    # 6.2 Implementación de PREDICT_PROBA adaptada a SciPy\n",
    "    # Sobrescribimos predict_proba para usar el método .logpdf() de SciPy\n",
    "    def predict_proba(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        log_prob_matrix = np.zeros((n_samples, len(self.classes_)))\n",
    "        \n",
    "        for idx, c in enumerate(self.classes_):\n",
    "            # Sumar el log-prior (log(P(y)))\n",
    "            log_prob_matrix[:, idx] = self.class_log_prior_[c]\n",
    "            \n",
    "            n_features = X.shape[1]\n",
    "            for i in range(n_features):\n",
    "                feature_data = X.iloc[:, i].values\n",
    "                \n",
    "                # Usar logpdf() de SciPy para obtener la log-verosimilitud\n",
    "                # .logpdf() toma las muestras X y devuelve el logaritmo de la densidad estimada\n",
    "                log_likelihoods = self.feature_estimators_[c][i].logpdf(feature_data)\n",
    "                \n",
    "                # log(P(y | x)) = log(P(y)) + SUM(log(P(x_i | y)))\n",
    "                log_prob_matrix[:, idx] += log_likelihoods\n",
    "\n",
    "        # Conversión de log-probabilidades a probabilidades (código estándar del Paso 5)\n",
    "        max_log_prob = np.max(log_prob_matrix, axis=1, keepdims=True)\n",
    "        exp_prob = np.exp(log_prob_matrix - max_log_prob)\n",
    "        prob_matrix = exp_prob / np.sum(exp_prob, axis=1, keepdims=True)\n",
    "        \n",
    "        return prob_matrix\n",
    "\n",
    "# 6.3 Evaluación del Modelo con Regla de Silverman\n",
    "modelo_silverman = CustomNaiveBayesKDE_Silverman()\n",
    "modelo_silverman.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba_silverman = modelo_silverman.predict_proba(X_test)[:, 1]\n",
    "auc_roc_silverman = roc_auc_score(y_test, y_pred_proba_silverman)\n",
    "\n",
    "print(\"--- Evaluación de KDE con Regla de Silverman ---\")\n",
    "print(f\"AUC ROC (KDE Silverman): {auc_roc_silverman:.4f}\")\n",
    "\n",
    "# Almacenar el resultado\n",
    "resultados['KDE_Silverman'] = auc_roc_silverman\n",
    "print(\"\\nResultados actuales para comparación:\")\n",
    "print(pd.Series(resultados))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
