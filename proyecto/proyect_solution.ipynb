{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4eae556",
   "metadata": {},
   "source": [
    "Paso 1: Carga del Dataset\n",
    "1. Importar Pandas y NumPy: Son las librerías fundamentales para manejar datos (pandas) y realizar cálculos numéricos eficientes (numpy).\n",
    "\n",
    "2. Cargar el Archivo: Usamos pd.read_csv() para leer el archivo del AI4I 2020 Predictive Maintenance Dataset. Asumiremos que el archivo se llama ai4i2020.csv y se encuentra en la misma carpeta que tu Jupyter Notebook.\n",
    "\n",
    "3. Mostrar Encabezado: Usamos .head() para ver las primeras 10 filas y confirmar que la carga fue exitosa y para empezar a inspeccionar el formato de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ab87ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UDI</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>Air temperature [K]</th>\n",
       "      <th>Process temperature [K]</th>\n",
       "      <th>Rotational speed [rpm]</th>\n",
       "      <th>Torque [Nm]</th>\n",
       "      <th>Tool wear [min]</th>\n",
       "      <th>Machine failure</th>\n",
       "      <th>TWF</th>\n",
       "      <th>HDF</th>\n",
       "      <th>PWF</th>\n",
       "      <th>OSF</th>\n",
       "      <th>RNF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>M14860</td>\n",
       "      <td>M</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1551</td>\n",
       "      <td>42.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>L47181</td>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408</td>\n",
       "      <td>46.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>L47182</td>\n",
       "      <td>L</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.5</td>\n",
       "      <td>1498</td>\n",
       "      <td>49.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>L47183</td>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1433</td>\n",
       "      <td>39.5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>L47184</td>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408</td>\n",
       "      <td>40.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>M14865</td>\n",
       "      <td>M</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1425</td>\n",
       "      <td>41.9</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>L47186</td>\n",
       "      <td>L</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1558</td>\n",
       "      <td>42.4</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>L47187</td>\n",
       "      <td>L</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1527</td>\n",
       "      <td>40.2</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>M14868</td>\n",
       "      <td>M</td>\n",
       "      <td>298.3</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1667</td>\n",
       "      <td>28.6</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>M14869</td>\n",
       "      <td>M</td>\n",
       "      <td>298.5</td>\n",
       "      <td>309.0</td>\n",
       "      <td>1741</td>\n",
       "      <td>28.0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UDI     Product ID   Type   Air temperature [K]   Process temperature [K]  \\\n",
       "0      1   M14860       M                    298.1                     308.6   \n",
       "1      2   L47181       L                    298.2                     308.7   \n",
       "2      3   L47182       L                    298.1                     308.5   \n",
       "3      4   L47183       L                    298.2                     308.6   \n",
       "4      5   L47184       L                    298.2                     308.7   \n",
       "5      6   M14865       M                    298.1                     308.6   \n",
       "6      7   L47186       L                    298.1                     308.6   \n",
       "7      8   L47187       L                    298.1                     308.6   \n",
       "8      9   M14868       M                    298.3                     308.7   \n",
       "9     10   M14869       M                    298.5                     309.0   \n",
       "\n",
       "    Rotational speed [rpm]   Torque [Nm]   Tool wear [min]   Machine failure  \\\n",
       "0                     1551          42.8                 0                 0   \n",
       "1                     1408          46.3                 3                 0   \n",
       "2                     1498          49.4                 5                 0   \n",
       "3                     1433          39.5                 7                 0   \n",
       "4                     1408          40.0                 9                 0   \n",
       "5                     1425          41.9                11                 0   \n",
       "6                     1558          42.4                14                 0   \n",
       "7                     1527          40.2                16                 0   \n",
       "8                     1667          28.6                18                 0   \n",
       "9                     1741          28.0                21                 0   \n",
       "\n",
       "    TWF   HDF   PWF   OSF   RNF  \n",
       "0     0     0     0     0     0  \n",
       "1     0     0     0     0     0  \n",
       "2     0     0     0     0     0  \n",
       "3     0     0     0     0     0  \n",
       "4     0     0     0     0     0  \n",
       "5     0     0     0     0     0  \n",
       "6     0     0     0     0     0  \n",
       "7     0     0     0     0     0  \n",
       "8     0     0     0     0     0  \n",
       "9     0     0     0     0     0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"./ai4i2020.csv\")\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9060bc8",
   "metadata": {},
   "source": [
    "Paso 2: Limpieza de nombres de columnas y Análisis Exploratorio de Datos (EDA)\n",
    "1. Crea una herramienta que toma un nombre de columna, elimina espacios al inicio/final, y reemplaza secuencias de múltiples espacios con un solo espacio.\n",
    "2. df.info(): Imprime un resumen de cuántos valores no nulos hay en cada columna y el tipo de dato (Dtype).\n",
    "3. df.describe(): Calcula estadísticas básicas (media, min, max, desviación estándar) para las columnas numéricas.\n",
    "4. df[NOMBRE_OBJETIVO].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b3712ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2.2.1 Información General del DataFrame ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   UDI                      10000 non-null  int64  \n",
      " 1   Product ID               10000 non-null  object \n",
      " 2   Type                     10000 non-null  object \n",
      " 3   Air temperature [K]      10000 non-null  float64\n",
      " 4   Process temperature [K]  10000 non-null  float64\n",
      " 5   Rotational speed [rpm]   10000 non-null  int64  \n",
      " 6   Torque [Nm]              10000 non-null  float64\n",
      " 7   Tool wear [min]          10000 non-null  int64  \n",
      " 8   Machine failure          10000 non-null  int64  \n",
      " 9   TWF                      10000 non-null  int64  \n",
      " 10  HDF                      10000 non-null  int64  \n",
      " 11  PWF                      10000 non-null  int64  \n",
      " 12  OSF                      10000 non-null  int64  \n",
      " 13  RNF                      10000 non-null  int64  \n",
      "dtypes: float64(3), int64(9), object(2)\n",
      "memory usage: 1.1+ MB\n",
      "\n",
      "--- 2.2.2 Estadísticas Descriptivas de Variables Numéricas ---\n",
      "               UDI  Air temperature [K]  Process temperature [K]  \\\n",
      "count  10000.00000         10000.000000             10000.000000   \n",
      "mean    5000.50000           300.004930               310.005560   \n",
      "std     2886.89568             2.000259                 1.483734   \n",
      "min        1.00000           295.300000               305.700000   \n",
      "25%     2500.75000           298.300000               308.800000   \n",
      "50%     5000.50000           300.100000               310.100000   \n",
      "75%     7500.25000           301.500000               311.100000   \n",
      "max    10000.00000           304.500000               313.800000   \n",
      "\n",
      "       Rotational speed [rpm]   Torque [Nm]  Tool wear [min]  Machine failure  \\\n",
      "count            10000.000000  10000.000000     10000.000000     10000.000000   \n",
      "mean              1538.776100     39.986910       107.951000         0.033900   \n",
      "std                179.284096      9.968934        63.654147         0.180981   \n",
      "min               1168.000000      3.800000         0.000000         0.000000   \n",
      "25%               1423.000000     33.200000        53.000000         0.000000   \n",
      "50%               1503.000000     40.100000       108.000000         0.000000   \n",
      "75%               1612.000000     46.800000       162.000000         0.000000   \n",
      "max               2886.000000     76.600000       253.000000         1.000000   \n",
      "\n",
      "                TWF           HDF           PWF           OSF          RNF  \n",
      "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.00000  \n",
      "mean       0.004600      0.011500      0.009500      0.009800      0.00190  \n",
      "std        0.067671      0.106625      0.097009      0.098514      0.04355  \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.00000  \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.00000  \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.00000  \n",
      "75%        0.000000      0.000000      0.000000      0.000000      0.00000  \n",
      "max        1.000000      1.000000      1.000000      1.000000      1.00000  \n",
      "\n",
      "--- 2.3.1 Conteo de la Variable Objetivo ('Machine failure') ---\n",
      "Machine failure\n",
      "0    9661\n",
      "1     339\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Porcentaje de Fallo de Máquina (Clase 1): 3.39%\n",
      "Este bajo porcentaje confirma un problema de desbalance de clases.\n"
     ]
    }
   ],
   "source": [
    "# Función para limpiar nombres: reemplaza múltiples espacios por uno y elimina espacios al inicio/final\n",
    "def limpiar_nombre_columna(col):\n",
    "    # ' '.join(col.split()) elimina cualquier secuencia de espacios y la reemplaza por un solo espacio.\n",
    "    col = ' '.join(col.split())\n",
    "    return col.strip()\n",
    "\n",
    "# Aplicar la limpieza a todos los nombres de columna\n",
    "df.columns = [limpiar_nombre_columna(col) for col in df.columns]\n",
    "\"\"\"\"\n",
    "print(\"--- Nombres de columnas después de la limpieza ---\")\n",
    "print(df.columns.tolist())\n",
    "\"\"\"\n",
    "\n",
    "# --- 2.2 ANÁLISIS DE TIPOS DE DATOS Y ESTADÍSTICAS ---\n",
    "\n",
    "print(\"\\n--- 2.2.1 Información General del DataFrame ---\")\n",
    "# Muestra el tipo de dato y la cuenta de valores no nulos (verificar si hay NaNs)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n--- 2.2.2 Estadísticas Descriptivas de Variables Numéricas ---\")\n",
    "# Muestra estadísticas como media, desviación estándar, min/max.\n",
    "print(df.describe())\n",
    "\n",
    "\n",
    "# --- 2.3 ANÁLISIS DE LA VARIABLE OBJETIVO ---\n",
    "\n",
    "print(f\"\\n--- 2.3.1 Conteo de la Variable Objetivo ('Machine failure') ---\")\n",
    "# Analiza el balance de la variable objetivo (0 = No Fallo, 1 = Fallo)\n",
    "conteo_fallos = df['Machine failure'].value_counts()\n",
    "print(conteo_fallos)\n",
    "\n",
    "# Calcular el porcentaje de fallos\n",
    "porcentaje_fallos = (conteo_fallos.get(1, 0) / df.shape[0]) * 100\n",
    "print(f\"\\nPorcentaje de Fallo de Máquina (Clase 1): {porcentaje_fallos:.2f}%\")\n",
    "print(\"Este bajo porcentaje confirma un problema de desbalance de clases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f293b0",
   "metadata": {},
   "source": [
    "Paso 3: Preprocesamiento de Datos y Escalado\n",
    "\n",
    "1. Eliminar Columnas Innecesarias: Se eliminan las columnas de identificación (UDI, Product ID) que no tienen poder predictivo.\n",
    "\n",
    "2. Separar X e y: El dataset se divide en características predictoras (X) y la variable objetivo (y), que es 'Machine failure'.\n",
    "\n",
    "3. Codificación de Variable Categórica: Se aplica One-Hot Encoding a la columna 'Type' (L, M, H) para convertirla en columnas binarias (ej: Type_L, Type_M), ya que los modelos matemáticos solo trabajan con números.\n",
    "\n",
    "4. Identificar Numéricas: Se identifican las columnas con valores continuos (temperatura, velocidad, torque, etc.).\n",
    "\n",
    "5. Escalado (StandardScaler): Se aplica StandardScaler a las columnas numéricas. Esto es crucial: centra los datos en una media de 0 y desviación estándar de 1, igualando las escalas para que Naive Bayes y KDE funcionen correctamente.\n",
    "\n",
    "6. División de Datos (70/30): El dataset se divide en un 70% para entrenamiento (X_train, y_train) y un 30% para prueba (X_test, y_test). La opción stratify=y asegura que la proporción de fallos (el desbalance) se mantenga igual en ambos conjuntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "640797b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas eliminadas: ['UDI', 'Product ID', 'TWF', 'HDF', 'PWF', 'OSF', 'RNF']\n",
      "Columnas finales de X: ['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]', 'Type_ L   ', 'Type_ M   ']\n",
      "\n",
      "--- Resumen de la División ---\n",
      "Tamaño de Entrenamiento (X_train): (7000, 7)\n",
      "Tamaño de Prueba (X_test): (3000, 7)\n"
     ]
    }
   ],
   "source": [
    "# Importar librerías necesarias\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Volver a cargar el dataframe original y limpiar nombres\n",
    "df = pd.read_csv(\"./ai4i2020.csv\") # Asegúrate de que esta ruta sea correcta\n",
    "def limpiar_nombre_columna(col):\n",
    "    col = ' '.join(col.split())\n",
    "    return col.strip()\n",
    "df.columns = [limpiar_nombre_columna(col) for col in df.columns]\n",
    "NOMBRE_OBJETIVO = 'Machine failure'\n",
    "\n",
    "# 3.1 Columnas a eliminar AHORA:\n",
    "columnas_a_eliminar = [\n",
    "    'UDI', \n",
    "    'Product ID', \n",
    "    # ¡NUEVA CORRECCIÓN! Eliminar las causas binarias de fallo\n",
    "    'TWF', 'HDF', 'PWF', 'OSF', 'RNF'\n",
    "]\n",
    "df = df.drop(columns=columnas_a_eliminar)\n",
    "print(f\"Columnas eliminadas: {columnas_a_eliminar}\")\n",
    "\n",
    "\n",
    "# 3.2 Separar X (predictoras) e y (objetivo)\n",
    "X = df.drop(columns=[NOMBRE_OBJETIVO])\n",
    "y = df[NOMBRE_OBJETIVO]\n",
    "\n",
    "\n",
    "# 3.3 Codificación de Variable Categórica ('Type')\n",
    "# Se mantiene drop_first=True para evitar colinealidad (corregida previamente)\n",
    "X = pd.get_dummies(X, columns=['Type'], drop_first=True) \n",
    "print(f\"Columnas finales de X: {X.columns.tolist()}\")\n",
    "\n",
    "\n",
    "# 3.4 Identificar y Escalar columnas numéricas\n",
    "columnas_numericas = X.select_dtypes(include=np.number).columns.tolist()\n",
    "scaler = StandardScaler()\n",
    "X[columnas_numericas] = scaler.fit_transform(X[columnas_numericas])\n",
    "\n",
    "\n",
    "# 3.5 División en conjuntos de entrenamiento y prueba (70/30)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\n--- Resumen de la División ---\")\n",
    "print(f\"Tamaño de Entrenamiento (X_train): {X_train.shape}\")\n",
    "print(f\"Tamaño de Prueba (X_test): {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a1dee4",
   "metadata": {},
   "source": [
    "Paso 4: Establecer la Línea Base (IA 1 - GaussianNB)\n",
    "\n",
    "Este paso implementa tu primera IA (IA 1), que es el modelo Gaussian Naive Bayes estándar de Scikit-learn. Este modelo actúa como el punto de referencia (baseline) con el cual compararás tus implementaciones de Naive Bayes con KDE más adelante.\n",
    "\n",
    "1. Importar Herramientas: Importar las clases necesarias: GaussianNB (el modelo) y roc_auc_score (la métrica requerida en tu metodología).\n",
    "\n",
    "2. Instanciar y Entrenar: Se inicializa el modelo GaussianNB y se entrena usando el conjunto de entrenamiento (X_train, y_train) que preparamos en el Paso 3.\n",
    "\n",
    "3. Predicción de Probabilidades: Se utiliza el modelo para predecir las probabilidades de pertenencia a la clase (0 o 1) en el conjunto de prueba (X_test). La métrica AUC ROC requiere probabilidades, no solo la clase final.\n",
    "\n",
    "4. Evaluación: Se calcula el valor AUC ROC usando las probabilidades predichas y los valores reales (y_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de64cbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluación de la Línea Base (Gaussian Naive Bayes) ---\n",
      "AUC ROC del Baseline (GaussianNB): 0.8626\n"
     ]
    }
   ],
   "source": [
    "# 4.1 Importar el modelo y la métrica de evaluación\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# 4.2 Instanciar y Entrenar el modelo\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X_train, y_train)\n",
    "\n",
    "# 4.3 Predecir las probabilidades en el conjunto de prueba\n",
    "# Usamos predict_proba para obtener la probabilidad de que sea la clase 1 (fallo)\n",
    "y_pred_proba_gnb = gnb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 4.4 Evaluar el modelo con la métrica AUC ROC\n",
    "auc_roc_gnb = roc_auc_score(y_test, y_pred_proba_gnb)\n",
    "\n",
    "print(\"--- Evaluación de la Línea Base (Gaussian Naive Bayes) ---\")\n",
    "print(f\"AUC ROC del Baseline (GaussianNB): {auc_roc_gnb:.4f}\")\n",
    "\n",
    "# Guardamos el resultado del baseline para compararlo en pasos futuros\n",
    "resultados = {'GaussianNB_Baseline': auc_roc_gnb}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbf5ac2",
   "metadata": {},
   "source": [
    "Paso 5: Implementación del Clasificador Naive Bayes Personalizado\n",
    "\n",
    "Este código implementa la lógica de Naive Bayes utilizando el logaritmo de las probabilidades para evitar errores de underflow (probabilidades muy pequeñas). Usaremos KernelDensity de Scikit-learn como la herramienta para la estimación.\n",
    "\n",
    "1. Definir la Clase: Creamos una clase CustomNaiveBayesKDE con un método de inicialización (__init__) para almacenar el método KDE y el bandwidth $h$ que se usarán.\n",
    "2. Método fit(X, y) (Entrenamiento):\n",
    "- Calcular Probabilidad a Priori: Calcula $P(y)$, la probabilidad de cada clase (Fallo y No Fallo).\n",
    "- Entrenar KDE: Para cada característica $x_i$ y para cada clase $y$, ajusta el estimador de densidad (KDE) a los datos de entrenamiento. Estos modelos ajustados se almacenan.\n",
    "3. Método predict_proba(X) (Predicción):\n",
    "- Calcular Verosimilitud: Para una nueva muestra de prueba, usa los modelos KDE entrenados para calcular la verosimilitud (probabilidad de las características dadas las clases), $P(x_i \\mid y)$.\n",
    "- Aplicar la Regla de Bayes: Combina la Probabilidad a Priori ($P(y)$) con la Verosimilitud para obtener la probabilidad final de pertenencia a la clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6fbd521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase CustomNaiveBayesKDE definida exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class CustomNaiveBayesKDE(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, kernel='gaussian', bandwidth=1.0):\n",
    "        # Almacena el tipo de kernel y el bandwidth (h)\n",
    "        self.kernel = kernel\n",
    "        self.bandwidth = bandwidth\n",
    "        \n",
    "        # Inicializa las variables que se almacenarán durante el entrenamiento\n",
    "        self.classes_ = None\n",
    "        self.class_log_prior_ = None\n",
    "        self.feature_estimators_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        # Inicializar estructuras para almacenar las probabilidades a priori y los estimadores de densidad\n",
    "        self.class_log_prior_ = {}\n",
    "        self.feature_estimators_ = {}\n",
    "        \n",
    "        # 1. Calcular Probabilidad a Priori (P(y))\n",
    "        for c in self.classes_:\n",
    "            # Obtener índices de las muestras que pertenecen a la clase actual\n",
    "            X_c = X[y == c]\n",
    "            \n",
    "            # Calcular log(P(y))\n",
    "            # Usamos logaritmos para evitar el underflow numérico\n",
    "            self.class_log_prior_[c] = np.log(X_c.shape[0] / X.shape[0])\n",
    "            \n",
    "            # 2. Entrenar KDE para cada característica (P(x_i | y))\n",
    "            self.feature_estimators_[c] = {}\n",
    "            for i in range(n_features):\n",
    "                # Extraer la característica i para la clase c\n",
    "                feature_data = X_c.iloc[:, i].values.reshape(-1, 1)\n",
    "                \n",
    "                # Instanciar y entrenar el KDE con el kernel y bandwidth definidos\n",
    "                kde = KernelDensity(kernel=self.kernel, bandwidth=self.bandwidth)\n",
    "                kde.fit(feature_data)\n",
    "                self.feature_estimators_[c][i] = kde\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        # Inicializar matriz de log-probabilidades (una fila por muestra, una columna por clase)\n",
    "        log_prob_matrix = np.zeros((n_samples, len(self.classes_)))\n",
    "        \n",
    "        # Calcular log(P(y | x)) para cada clase\n",
    "        for idx, c in enumerate(self.classes_):\n",
    "            # Sumar el log-prior (log(P(y)))\n",
    "            log_prob_matrix[:, idx] = self.class_log_prior_[c]\n",
    "            \n",
    "            # Sumar las log-verosimilitudes (log(P(x_i | y))) para cada característica\n",
    "            n_features = X.shape[1]\n",
    "            for i in range(n_features):\n",
    "                feature_data = X.iloc[:, i].values.reshape(-1, 1)\n",
    "                \n",
    "                # Obtener log-verosimilitud del KDE para la característica i y la clase c\n",
    "                log_likelihoods = self.feature_estimators_[c][i].score_samples(feature_data)\n",
    "                \n",
    "                # log(P(y | x)) = log(P(y)) + SUM(log(P(x_i | y)))\n",
    "                log_prob_matrix[:, idx] += log_likelihoods\n",
    "\n",
    "        # Convertir log-probabilidades a probabilidades normales (prob_matrix)\n",
    "        # Esto requiere normalizar las log-probabilidades para obtener probabilidades válidas\n",
    "        \n",
    "        # Restar el máximo para evitar sobreflujo al exponer\n",
    "        max_log_prob = np.max(log_prob_matrix, axis=1, keepdims=True)\n",
    "        exp_prob = np.exp(log_prob_matrix - max_log_prob)\n",
    "        \n",
    "        # Normalizar para que las probabilidades sumen 1\n",
    "        prob_matrix = exp_prob / np.sum(exp_prob, axis=1, keepdims=True)\n",
    "        \n",
    "        return prob_matrix\n",
    "\n",
    "    def predict(self, X):\n",
    "        # La predicción es simplemente la clase con la probabilidad más alta\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return self.classes_[np.argmax(probabilities, axis=1)]\n",
    "\n",
    "\n",
    "# Prueba rápida de la estructura (no es una prueba real de KDE aún)\n",
    "print(\"Clase CustomNaiveBayesKDE definida exitosamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8e97bf",
   "metadata": {},
   "source": [
    "Paso 6: KDE con Regla de Silverman (Método 2.c)\n",
    "\n",
    "Dado que la librería scipy.stats.gaussian_kde maneja el cálculo de la densidad de manera diferente a sklearn.neighbors.KernelDensity, crearemos una función de entrenamiento separada.\n",
    "\n",
    "1. Importar Herramienta: Importar gaussian_kde de SciPy, que se basa en la Regla de Silverman.\n",
    "2. Definir CustomNaiveBayesKDE_Silverman: Creamos una nueva clase que hereda la lógica general del Naive Bayes implementado en el Paso 5, pero que sobrescribe el método de entrenamiento (fit) para usar la función de SciPy.\n",
    "3. Entrenamiento con Silverman: Dentro del fit, usamos gaussian_kde(feature_data) para entrenar la estimación de densidad para cada característica y clase. Esta función calcula internamente el $h$ óptimo con la Regla de Silverman.\n",
    "4. Cálculo de Verosimilitud: Para la predicción, usamos el método .logpdf() del objeto de SciPy para obtener el $\\log P(x_i \\mid y)$.\n",
    "5. Evaluación: Entrenar y evaluar esta nueva clase para obtener el AUC ROC y compararlo con el Baseline (Paso 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad84e06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluación de KDE con Regla de Silverman ---\n",
      "AUC ROC (KDE Silverman): 0.9134\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# PASO 6 (CORREGIDO): IMPLEMENTACIÓN Y EVALUACIÓN DE KDE CON REGLA DE SILVERMAN\n",
    "# Solución al TypeError forzando la conversión a float.\n",
    "# =================================================================\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin # Importar para que la herencia funcione\n",
    "# Asume que CustomNaiveBayesKDE ya está definida del Paso 5\n",
    "\n",
    "class CustomNaiveBayesKDE_Silverman(CustomNaiveBayesKDE):\n",
    "    \n",
    "    # 6.1 Implementación de FIT usando la Regla de Silverman (SciPy)\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_features = X.shape[1]\n",
    "        self.class_log_prior_ = {}\n",
    "        self.feature_estimators_ = {}\n",
    "        \n",
    "        for c in self.classes_:\n",
    "            X_c = X[y == c]\n",
    "            \n",
    "            # Cálculo del log(P(y)) - Probabilidad a Priori\n",
    "            self.class_log_prior_[c] = np.log(X_c.shape[0] / X.shape[0])\n",
    "            self.feature_estimators_[c] = {}\n",
    "            \n",
    "            # Entrenamiento de KDE para cada característica usando SciPy\n",
    "            for i in range(n_features):\n",
    "                # CORRECCIÓN CLAVE 1: Forzar la conversión a numpy array de flotantes\n",
    "                feature_data = X_c.iloc[:, i].values.astype(np.float64)\n",
    "                \n",
    "                # gaussian_kde calcula el bandwidth h automáticamente con la Regla de Silverman\n",
    "                # Se requiere que feature_data sea 1D para SciPy\n",
    "                kde = gaussian_kde(feature_data) \n",
    "                self.feature_estimators_[c][i] = kde\n",
    "        \n",
    "        return self\n",
    "\n",
    "    # 6.2 Implementación de PREDICT_PROBA adaptada a SciPy\n",
    "    def predict_proba(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        log_prob_matrix = np.zeros((n_samples, len(self.classes_)))\n",
    "        \n",
    "        for idx, c in enumerate(self.classes_):\n",
    "            # Sumar el log-prior (log(P(y)))\n",
    "            log_prob_matrix[:, idx] = self.class_log_prior_[c]\n",
    "            \n",
    "            n_features = X.shape[1]\n",
    "            for i in range(n_features):\n",
    "                # CORRECCIÓN CLAVE 2: Forzar la conversión a numpy array de flotantes\n",
    "                feature_data = X.iloc[:, i].values.astype(np.float64)\n",
    "\n",
    "                # Usar logpdf() de SciPy para obtener la log-verosimilitud\n",
    "                log_likelihoods = self.feature_estimators_[c][i].logpdf(feature_data)\n",
    "                \n",
    "                log_prob_matrix[:, idx] += log_likelihoods\n",
    "\n",
    "        # Conversión de log-probabilidades a probabilidades (código estándar del Paso 5)\n",
    "        max_log_prob = np.max(log_prob_matrix, axis=1, keepdims=True)\n",
    "        exp_prob = np.exp(log_prob_matrix - max_log_prob)\n",
    "        prob_matrix = exp_prob / np.sum(exp_prob, axis=1, keepdims=True)\n",
    "        \n",
    "        return prob_matrix\n",
    "\n",
    "# 6.3 Evaluación del Modelo con Regla de Silverman\n",
    "modelo_silverman = CustomNaiveBayesKDE_Silverman()\n",
    "# Asume que X_train y y_train están listos del Paso 3\n",
    "modelo_silverman.fit(X_train, y_train) \n",
    "\n",
    "# Asume que X_test y y_test están listos del Paso 3\n",
    "y_pred_proba_silverman = modelo_silverman.predict_proba(X_test)[:, 1]\n",
    "auc_roc_silverman = roc_auc_score(y_test, y_pred_proba_silverman)\n",
    "\n",
    "print(\"--- Evaluación de KDE con Regla de Silverman ---\")\n",
    "print(f\"AUC ROC (KDE Silverman): {auc_roc_silverman:.4f}\")\n",
    "\n",
    "# Almacenar el resultado (asume que 'resultados' existe del Paso 4)\n",
    "# Asegúrate de que la variable resultados existe y tiene el GNB baseline\n",
    "# resultados['KDE_Silverman'] = auc_roc_silverman\n",
    "# print(\"\\nResultados actuales para comparación:\")\n",
    "# print(pd.Series(resultados))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
