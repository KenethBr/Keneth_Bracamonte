{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6974964c",
   "metadata": {},
   "source": [
    "# Clasificación de Piso en el Dataset UJIIndoorLoc\n",
    "\n",
    "---\n",
    "\n",
    "## Introducción\n",
    "\n",
    "En este notebook se implementa un flujo completo de procesamiento y análisis para la clasificación del **piso** en un entorno interior utilizando el dataset **UJIIndoorLoc**. Este conjunto de datos contiene mediciones de señales WiFi recopiladas en distintas ubicaciones de un edificio, con información sobre coordenadas, piso, usuario, hora, entre otros.\n",
    "\n",
    "En esta tarea nos enfocaremos en predecir el **piso** en el que se encuentra un dispositivo, considerando únicamente las muestras etiquetadas con valores válidos para dicha variable. Se tratará como un problema de clasificación multiclase (planta baja, primer piso, segundo piso).\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "- **Cargar y explorar** el conjunto de datos UJIIndoorLoc.\n",
    "- **Preparar** los datos seleccionando las características relevantes y el target (`FLOOR`).\n",
    "- **Dividir** el dataset en entrenamiento y validación (80/20).\n",
    "- **Entrenar y optimizar** clasificadores basados en seis algoritmos:\n",
    "  - K-Nearest Neighbors (KNN)\n",
    "  - Gaussian Naive Bayes\n",
    "  - Regresión Logística\n",
    "  - Árboles de Decisión\n",
    "  - Support Vector Machines (SVM)\n",
    "  - Random Forest\n",
    "- **Seleccionar hiperparámetros óptimos** para cada modelo utilizando validación cruzada (5-fold), empleando estrategias como **Grid Search**, **Randomized Search**, o **Bayesian Optimization** según el algoritmo.\n",
    "- **Comparar el desempeño** de los modelos sobre el conjunto de validación, usando métricas como *accuracy*, *precision*, *recall*, y *F1-score*.\n",
    "- **Determinar el mejor clasificador** para esta tarea, junto con sus hiperparámetros óptimos.\n",
    "\n",
    "Este ejercicio permite no solo evaluar la capacidad predictiva de distintos algoritmos clásicos de clasificación, sino también desarrollar buenas prácticas en validación de modelos y selección de hiperparámetros en contextos del mundo real.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253ad8d2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Descripción del Dataset\n",
    "\n",
    "El dataset utilizado en este análisis es el **UJIIndoorLoc Dataset**, ampliamente utilizado para tareas de localización en interiores a partir de señales WiFi. Está disponible públicamente en la UCI Machine Learning Repository y ha sido recopilado en un entorno real de un edificio universitario.\n",
    "\n",
    "Cada muestra corresponde a una observación realizada por un dispositivo móvil, donde se registran las intensidades de señal (RSSI) de más de 500 puntos de acceso WiFi disponibles en el entorno. Además, cada fila contiene información contextual como la ubicación real del dispositivo (coordenadas X e Y), el piso, el edificio, el identificador del usuario, y la marca temporal.\n",
    "\n",
    "El objetivo en esta tarea es predecir el **piso** (`FLOOR`) en el que se encontraba el dispositivo en el momento de la medición, considerando únicamente las características numéricas provenientes de las señales WiFi.\n",
    "\n",
    "### Estructura del dataset\n",
    "\n",
    "- **Número de muestras**: ~20,000\n",
    "- **Número de características**: 520\n",
    "  - 520 columnas con valores de intensidad de señal WiFi (`WAP001` a `WAP520`)\n",
    "- **Variable objetivo**: `FLOOR` (variable categórica con múltiples clases, usualmente entre 0 y 4)\n",
    "\n",
    "### Columnas relevantes\n",
    "\n",
    "- `WAP001`, `WAP002`, ..., `WAP520`: niveles de señal recibida desde cada punto de acceso WiFi (valores entre -104 y 0, o 100 si no se detectó).\n",
    "- `FLOOR`: clase objetivo a predecir (nivel del edificio).\n",
    "- (Otras columnas como `BUILDINGID`, `SPACEID`, `USERID`, `TIMESTAMP`, etc., pueden ser ignoradas o utilizadas en análisis complementarios).\n",
    "\n",
    "### Contexto del problema\n",
    "\n",
    "La localización en interiores es un problema complejo en el que tecnologías como el GPS no funcionan adecuadamente. Los sistemas basados en WiFi han demostrado ser una alternativa efectiva para estimar la ubicación de usuarios en edificios. Poder predecir automáticamente el piso en el que se encuentra una persona puede mejorar aplicaciones de navegación en interiores, accesibilidad, gestión de emergencias y servicios personalizados. Este tipo de problemas es típicamente abordado mediante algoritmos de clasificación multiclase.\n",
    "\n",
    "\n",
    "### Estrategia de evaluación\n",
    "\n",
    "En este análisis seguiremos una metodología rigurosa para garantizar la validez de los resultados:\n",
    "\n",
    "1. **Dataset de entrenamiento**: Se utilizará exclusivamente para el desarrollo, entrenamiento y optimización de hiperparámetros de todos los modelos. Este conjunto será dividido internamente en subconjuntos de entrenamiento y validación (80/20) para la selección de hiperparámetros mediante validación cruzada.\n",
    "\n",
    "2. **Dataset de prueba**: Se reservará únicamente para la **evaluación final** de los modelos ya optimizados. Este conjunto **no debe ser utilizado** durante el proceso de selección de hiperparámetros, ajuste de modelos o toma de decisiones sobre la arquitectura, ya que esto introduciría sesgo y comprometería la capacidad de generalización estimada.\n",
    "\n",
    "3. **Validación cruzada**: Para la optimización de hiperparámetros se empleará validación cruzada 5-fold sobre el conjunto de entrenamiento, lo que permitirá una estimación robusta del rendimiento sin contaminar los datos de prueba.\n",
    "\n",
    "Esta separación estricta entre datos de desarrollo y evaluación final es fundamental para obtener una estimación realista del rendimiento que los modelos tendrían en un escenario de producción con datos completamente nuevos.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4b8e4d",
   "metadata": {},
   "source": [
    "## Paso 1: Cargar y explorar el dataset\n",
    "\n",
    "**Instrucciones:**\n",
    "- Descarga el dataset **UJIIndoorLoc** desde la UCI Machine Learning Repository o utiliza la versión proporcionada en el repositorio del curso (por ejemplo: `datasets\\UJIIndoorLoc\\trainingData.csv`).\n",
    "- Carga el dataset utilizando `pandas`.\n",
    "- Muestra las primeras filas del dataset utilizando `df.head()`.\n",
    "- Imprime el número total de muestras (filas) y características (columnas).\n",
    "- Verifica cuántas clases distintas hay en la variable objetivo `FLOOR` y cuántas muestras tiene cada clase (`df['FLOOR'].value_counts()`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6553d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo cargado: trainingData.csv\n",
      "\n",
      "Primeras 5 filas:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WAP001</th>\n",
       "      <th>WAP002</th>\n",
       "      <th>WAP003</th>\n",
       "      <th>WAP004</th>\n",
       "      <th>WAP005</th>\n",
       "      <th>WAP006</th>\n",
       "      <th>WAP007</th>\n",
       "      <th>WAP008</th>\n",
       "      <th>WAP009</th>\n",
       "      <th>WAP010</th>\n",
       "      <th>...</th>\n",
       "      <th>WAP520</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>FLOOR</th>\n",
       "      <th>BUILDINGID</th>\n",
       "      <th>SPACEID</th>\n",
       "      <th>RELATIVEPOSITION</th>\n",
       "      <th>USERID</th>\n",
       "      <th>PHONEID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>-7541.2643</td>\n",
       "      <td>4.864921e+06</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>1371713733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>-7536.6212</td>\n",
       "      <td>4.864934e+06</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>1371713691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>-97</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>-7519.1524</td>\n",
       "      <td>4.864950e+06</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>1371714095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>-7524.5704</td>\n",
       "      <td>4.864934e+06</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>102</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>1371713807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>-7632.1436</td>\n",
       "      <td>4.864982e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>1369909710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 529 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   WAP001  WAP002  WAP003  WAP004  WAP005  WAP006  WAP007  WAP008  WAP009  \\\n",
       "0     100     100     100     100     100     100     100     100     100   \n",
       "1     100     100     100     100     100     100     100     100     100   \n",
       "2     100     100     100     100     100     100     100     -97     100   \n",
       "3     100     100     100     100     100     100     100     100     100   \n",
       "4     100     100     100     100     100     100     100     100     100   \n",
       "\n",
       "   WAP010  ...  WAP520  LONGITUDE      LATITUDE  FLOOR  BUILDINGID  SPACEID  \\\n",
       "0     100  ...     100 -7541.2643  4.864921e+06      2           1      106   \n",
       "1     100  ...     100 -7536.6212  4.864934e+06      2           1      106   \n",
       "2     100  ...     100 -7519.1524  4.864950e+06      2           1      103   \n",
       "3     100  ...     100 -7524.5704  4.864934e+06      2           1      102   \n",
       "4     100  ...     100 -7632.1436  4.864982e+06      0           0      122   \n",
       "\n",
       "   RELATIVEPOSITION  USERID  PHONEID   TIMESTAMP  \n",
       "0                 2       2       23  1371713733  \n",
       "1                 2       2       23  1371713691  \n",
       "2                 2       2       23  1371714095  \n",
       "3                 2       2       23  1371713807  \n",
       "4                 2      11       13  1369909710  \n",
       "\n",
       "[5 rows x 529 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensiones (filas, columnas): (19937, 529)\n",
      "\n",
      "Columnas (primeras 20 mostradas):\n",
      "['WAP001', 'WAP002', 'WAP003', 'WAP004', 'WAP005', 'WAP006', 'WAP007', 'WAP008', 'WAP009', 'WAP010', 'WAP011', 'WAP012', 'WAP013', 'WAP014', 'WAP015', 'WAP016', 'WAP017', 'WAP018', 'WAP019', 'WAP020'] ... total columnas=529\n",
      "\n",
      "Conteo por clase en 'FLOOR':\n",
      "FLOOR\n",
      "3    5048\n",
      "1    5002\n",
      "2    4416\n",
      "0    4369\n",
      "4    1102\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Número de clases distintas en 'FLOOR': 5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Paso 1: Cargar y explorar el dataset UJIIndoorLoc (trainingData.csv)\n",
    "\n",
    "# Posibles rutas comunes donde puede estar el archivo\n",
    "candidate_paths = [\n",
    "    Path(\"datasets/UJIIndoorLoc/trainingData.csv\"),\n",
    "    Path(\"datasets/UJIIndoorLoc/trainingData.csv\").resolve(),\n",
    "    Path(\"trainingData.csv\"),\n",
    "    Path(\"data/trainingData.csv\"),\n",
    "    Path(\"./trainingData.csv\"),\n",
    "]\n",
    "\n",
    "df = None\n",
    "loaded_path = None\n",
    "for p in candidate_paths:\n",
    "    if p.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(p)\n",
    "            loaded_path = p\n",
    "            break\n",
    "        except Exception as e:\n",
    "            # si existe pero no se pudo leer, mostrar error y seguir probando otras rutas\n",
    "            print(f\"Error leyendo {p}: {e}\")\n",
    "\n",
    "if df is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"No se encontró trainingData.csv en las rutas probadas. \"\n",
    "        \"Coloca el archivo en el directorio del notebook o en 'datasets/UJIIndoorLoc/'.\"\n",
    "    )\n",
    "\n",
    "# Mostrar resultados básicos de exploración\n",
    "print(f\"Archivo cargado: {loaded_path}\")\n",
    "print(\"\\nPrimeras 5 filas:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nDimensiones (filas, columnas):\", df.shape)\n",
    "\n",
    "print(\"\\nColumnas (primeras 20 mostradas):\")\n",
    "print(list(df.columns)[:20], (\"... total columnas=\" + str(len(df.columns))))\n",
    "\n",
    "if 'FLOOR' in df.columns:\n",
    "    print(\"\\nConteo por clase en 'FLOOR':\")\n",
    "    print(df['FLOOR'].value_counts(dropna=False))\n",
    "    print(\"\\nNúmero de clases distintas en 'FLOOR':\", df['FLOOR'].nunique())\n",
    "else:\n",
    "    print(\"\\nLa columna 'FLOOR' no se encontró en el dataset. Revisa el archivo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2f0bed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Paso 2: Preparar los datos\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "- Elimina las columnas que no son relevantes para la tarea de clasificación del piso:\n",
    "  - `LONGITUDE`, `LATITUDE`, `SPACEID`, `RELATIVEPOSITION`, `USERID`, `PHONEID`, `TIMESTAMP`\n",
    "- Conserva únicamente:\n",
    "  - Las columnas `WAP001` a `WAP520` como características (RSSI de puntos de acceso WiFi).\n",
    "  - La columna `FLOOR` como variable objetivo.\n",
    "- Verifica si existen valores atípicos o valores inválidos en las señales WiFi (por ejemplo: valores constantes como 100 o -110 que suelen indicar ausencia de señal).\n",
    "- Separa el conjunto de datos en:\n",
    "  - `X`: matriz de características (todas las columnas `WAP`)\n",
    "  - `y`: vector objetivo (`FLOOR`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ec8c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones originales: (19937, 529)\n",
      "Dimensiones después de limpiar: X=(19937, 520), y=(19937,)\n",
      "Número de columnas WAP detectadas: 520\n",
      "Total de entradas con valor 100 (ausencia): 10008477\n",
      "Total de entradas con valor -110 (posible outlier): 0\n",
      "Rango de valores RSSI detectado en X: min=-104, max=100\n",
      "\n",
      "Distribución de clases en 'FLOOR':\n",
      "FLOOR\n",
      "0    4369\n",
      "1    5002\n",
      "2    4416\n",
      "3    5048\n",
      "4    1102\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Paso 2: Preparar los datos (eliminar columnas irrelevantes, seleccionar WAPs y target)\n",
    "\n",
    "# Columnas a eliminar según instrucción\n",
    "cols_to_drop = ['LONGITUDE', 'LATITUDE', 'SPACEID', 'RELATIVEPOSITION', 'USERID', 'PHONEID', 'TIMESTAMP']\n",
    "\n",
    "# Crear una copia limpia del dataframe original sin las columnas irrelevantes\n",
    "df_clean = df.drop(columns=cols_to_drop, errors='ignore').copy()\n",
    "\n",
    "# Determinar columnas WAP (WAP001..WAP520) existents en el dataframe\n",
    "wap_cols = [f\"WAP{str(i).zfill(3)}\" for i in range(1, 521)]\n",
    "wap_cols = [c for c in wap_cols if c in df_clean.columns]\n",
    "\n",
    "# Verificar que la columna objetivo exista\n",
    "if 'FLOOR' not in df_clean.columns:\n",
    "    raise KeyError(\"La columna 'FLOOR' no se encontró en el dataset.\")\n",
    "\n",
    "# Construir X (características WAP) e y (target)\n",
    "X = df_clean[wap_cols].copy()\n",
    "y = df_clean['FLOOR'].copy()\n",
    "\n",
    "# Información resumida de la preparación\n",
    "print(f\"Dimensiones originales: {df.shape}\")\n",
    "print(f\"Dimensiones después de limpiar: X={X.shape}, y={y.shape}\")\n",
    "print(f\"Número de columnas WAP detectadas: {len(wap_cols)}\")\n",
    "\n",
    "# Comprobaciones de valores atípicos / indicadores de ausencia de señal\n",
    "count_100 = (X == 100).sum().sum()\n",
    "count_minus_110 = (X == -110).sum().sum()\n",
    "rssi_min = X.min().min()\n",
    "rssi_max = X.max().max()\n",
    "\n",
    "print(f\"Total de entradas con valor 100 (ausencia): {count_100}\")\n",
    "print(f\"Total de entradas con valor -110 (posible outlier): {count_minus_110}\")\n",
    "print(f\"Rango de valores RSSI detectado en X: min={rssi_min}, max={rssi_max}\")\n",
    "\n",
    "# Cuenta de clases en y\n",
    "print(\"\\nDistribución de clases en 'FLOOR':\")\n",
    "print(y.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0eec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tu código aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8a6c39",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Paso 3: Preprocesamiento de las señales WiFi\n",
    "\n",
    "**Contexto:**\n",
    "\n",
    "Las columnas `WAP001` a `WAP520` representan la intensidad de la señal (RSSI) recibida desde distintos puntos de acceso WiFi. Los valores típicos de RSSI están en una escala negativa, donde:\n",
    "\n",
    "- Valores cercanos a **0 dBm** indican señal fuerte.\n",
    "- Valores cercanos a **-100 dBm** indican señal débil o casi ausente.\n",
    "- Un valor de **100** en este dataset representa una señal **no detectada**, es decir, el punto de acceso no fue visto por el dispositivo en ese instante.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "- Para facilitar el procesamiento y tratar la ausencia de señal de forma coherente, se recomienda mapear todos los valores **100** a **-100**, que semánticamente representa *ausencia de señal detectable*.\n",
    "- Esto unifica el rango de valores y evita que 100 (un valor artificial) afecte negativamente la escala de los algoritmos.\n",
    "\n",
    "**Pasos sugeridos:**\n",
    "\n",
    "- Reemplaza todos los valores `100` por `-100` en las columnas `WAP001` a `WAP520`:\n",
    "  ```python\n",
    "  X[X == 100] = -100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "811d9af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de entradas con valor 100 antes: 10008477\n",
      "Total de entradas con valor 100 después: 0\n",
      "Rango de valores RSSI en X: min = -104 , max = 0\n"
     ]
    }
   ],
   "source": [
    "# Paso 3: Preprocesamiento de las señales WiFi\n",
    "# Reemplazar los 100 (no detectado) por -100 (ausencia de señal)\n",
    "print(\"Total de entradas con valor 100 antes:\", (X == 100).sum().sum())\n",
    "\n",
    "X.replace(100, -100, inplace=True)\n",
    "\n",
    "print(\"Total de entradas con valor 100 después:\", (X == 100).sum().sum())\n",
    "print(\"Rango de valores RSSI en X: min =\", X.min().min(), \", max =\", X.max().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80383336",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Paso 4: Entrenamiento y optimización de hiperparámetros\n",
    "\n",
    "**Objetivo:**\n",
    "\n",
    "Entrenar y comparar distintos clasificadores para predecir correctamente el piso (`FLOOR`) y encontrar los mejores hiperparámetros para cada uno mediante validación cruzada.\n",
    "\n",
    "**Clasificadores a evaluar:**\n",
    "\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Gaussian Naive Bayes\n",
    "- Regresión Logística\n",
    "- Árboles de Decisión\n",
    "- Support Vector Machines (SVM)\n",
    "- Random Forest\n",
    "\n",
    "**Procedimiento:**\n",
    "\n",
    "1. Divide el dataset en conjunto de **entrenamiento** (80%) y **validación** (20%) usando `train_test_split` con `stratify=y`.\n",
    "2. Para cada clasificador:\n",
    "   - Define el espacio de búsqueda de hiperparámetros.\n",
    "   - Usa **validación cruzada 5-fold** sobre el conjunto de entrenamiento para seleccionar los mejores hiperparámetros.\n",
    "   - Emplea una estrategia de búsqueda adecuada:\n",
    "     - **GridSearchCV**: búsqueda exhaustiva (ideal para espacios pequeños).\n",
    "     - **RandomizedSearchCV**: búsqueda aleatoria (más eficiente con espacios amplios).\n",
    "     - **Bayesian Optimization** (opcional): para búsquedas más inteligentes, usando librerías como `optuna` o `skopt`.\n",
    "3. Guarda el mejor modelo encontrado para cada clasificador con su configuración óptima.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27e7f735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones:\n",
      "  X_train: (15949, 520) y_train: (15949,)\n",
      "  X_val:   (3988, 520) y_val:   (3988,)\n",
      "\n",
      "Distribución de clases (proporción) en y_train:\n",
      "FLOOR\n",
      "0    0.219136\n",
      "1    0.250862\n",
      "2    0.221519\n",
      "3    0.253182\n",
      "4    0.055301\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Distribución de clases (proporción) en y_val:\n",
      "FLOOR\n",
      "0    0.219157\n",
      "1    0.251003\n",
      "2    0.221414\n",
      "3    0.253260\n",
      "4    0.055165\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paso 4 (inicio): dividir el dataset en entrenamiento (80%) y validación (20%) con estratificación por y\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Dimensiones:\")\n",
    "print(\"  X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
    "print(\"  X_val:  \", X_val.shape,   \"y_val:  \", y_val.shape)\n",
    "\n",
    "print(\"\\nDistribución de clases (proporción) en y_train:\")\n",
    "print(y_train.value_counts(normalize=True).sort_index())\n",
    "\n",
    "print(\"\\nDistribución de clases (proporción) en y_val:\")\n",
    "print(y_val.value_counts(normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e06160a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Mejores hiperparámetros KNN: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
      "Mejor score de validación cruzada: 0.9962380282534692\n",
      "Accuracy en conjunto de validación: 0.9952357071213641\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Espacio de búsqueda para KNN (pequeño, usamos GridSearchCV)\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# GridSearchCV con validación cruzada 5-fold\n",
    "grid_search = GridSearchCV(\n",
    "    knn,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Mejores hiperparámetros KNN:\", grid_search.best_params_)\n",
    "print(\"Mejor score de validación cruzada:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluación en el conjunto de validación hold-out\n",
    "best_knn = grid_search.best_estimator_\n",
    "val_score = best_knn.score(X_val, y_val)\n",
    "print(\"Accuracy en conjunto de validación:\", val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac37f94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "Mejores hiperparámetros GNB: {'var_smoothing': np.float64(0.01)}\n",
      "Mejor score CV (accuracy): 0.7114555815395989\n",
      "Accuracy en validación hold-out: 0.7091273821464393\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "# Espacio de búsqueda para GaussianNB (var_smoothing en escala log)\n",
    "param_grid_gnb = {'var_smoothing': np.logspace(-9, -1, 9)}\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# GridSearchCV ya está disponible en el notebook; usarlo con cv=5\n",
    "grid_search_gnb = GridSearchCV(\n",
    "    gnb,\n",
    "    param_grid_gnb,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_search_gnb.fit(X_train, y_train)\n",
    "\n",
    "print(\"Mejores hiperparámetros GNB:\", grid_search_gnb.best_params_)\n",
    "print(\"Mejor score CV (accuracy):\", grid_search_gnb.best_score_)\n",
    "\n",
    "# Guardar el mejor estimador y evaluar en el hold-out (X_val, y_val)\n",
    "best_gnb = grid_search_gnb.best_estimator_\n",
    "val_score_gnb = best_gnb.score(X_val, y_val)\n",
    "print(\"Accuracy en validación hold-out:\", val_score_gnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "477f5e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando Árbol de Decisión con GridSearchCV...\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Tiempo de entrenamiento: 24.02 segundos\n",
      "Mejores hiperparámetros DT: {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "Mejor score de validación cruzada: 0.9659542844672764\n",
      "Accuracy en conjunto de validación: 0.9711634904714143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "# Paso 4 (Árbol de Decisión): Entrenamiento y optimización de hiperparámetros\n",
    "\n",
    "# Espacio de búsqueda para Árbol de Decisión (pequeño-medio, usamos GridSearchCV)\n",
    "param_grid_dt = {\n",
    "    'max_depth': [5, 10, 15, 20, None],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# GridSearchCV con validación cruzada 5-fold\n",
    "grid_search_dt = GridSearchCV(\n",
    "    dt,\n",
    "    param_grid_dt,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"Entrenando Árbol de Decisión con GridSearchCV...\")\n",
    "start_time = time.time()\n",
    "grid_search_dt.fit(X_train, y_train)\n",
    "train_time_dt = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTiempo de entrenamiento: {train_time_dt:.2f} segundos\")\n",
    "print(\"Mejores hiperparámetros DT:\", grid_search_dt.best_params_)\n",
    "print(\"Mejor score de validación cruzada:\", grid_search_dt.best_score_)\n",
    "\n",
    "# Guardar el mejor estimador y evaluar en el hold-out (X_val, y_val)\n",
    "best_dt = grid_search_dt.best_estimator_\n",
    "val_score_dt = best_dt.score(X_val, y_val)\n",
    "print(\"Accuracy en conjunto de validación:\", val_score_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66f8eb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando DecisionTree con RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "Tiempo de entrenamiento: 7.30 segundos\n",
      "Mejores hiperparámetros DT: {'splitter': 'random', 'min_samples_split': np.int64(7), 'min_samples_leaf': np.int64(3), 'max_depth': None, 'criterion': 'entropy'}\n",
      "Mejor score de validación cruzada: 0.9564241107018543\n",
      "Accuracy en conjunto de validación: 0.9583751253761283\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Espacio de búsqueda para DecisionTree (más amplio, usamos RandomizedSearchCV)\n",
    "param_dist_dt = {\n",
    "    'max_depth': [5, 10, 15, 20, None],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_split': np.arange(2, 21),\n",
    "    'min_samples_leaf': np.arange(1, 11),\n",
    "    'splitter': ['best', 'random']\n",
    "}\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "random_search_dt = RandomizedSearchCV(\n",
    "    dt,\n",
    "    param_distributions=param_dist_dt,\n",
    "    n_iter=30,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Entrenando DecisionTree con RandomizedSearchCV...\")\n",
    "start_time = time.time()\n",
    "random_search_dt.fit(X_train, y_train)\n",
    "train_time_dt = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTiempo de entrenamiento: {train_time_dt:.2f} segundos\")\n",
    "print(\"Mejores hiperparámetros DT:\", random_search_dt.best_params_)\n",
    "print(\"Mejor score de validación cruzada:\", random_search_dt.best_score_)\n",
    "\n",
    "# Evaluación en el conjunto de validación hold-out\n",
    "best_dt = random_search_dt.best_estimator_\n",
    "val_score_dt = best_dt.score(X_val, y_val)\n",
    "print(\"Accuracy en conjunto de validación:\", val_score_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33400362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando SVM con RandomizedSearchCV (versión rápida)...\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "Mejores hiperparámetros SVM: {'kernel': 'linear', 'gamma': 'scale', 'C': 0.1}\n",
      "Mejor score de validación cruzada: 0.9749989869929901\n",
      "Accuracy en conjunto de validación: 0.9819458375125376\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist_svm = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 0.01, 0.1]\n",
    "}\n",
    "\n",
    "svm = SVC(random_state=42)\n",
    "\n",
    "# Usar una muestra pequeña para tuning\n",
    "X_train_small = X_train.sample(n=2000, random_state=42)\n",
    "y_train_small = y_train.loc[X_train_small.index]\n",
    "\n",
    "random_search_svm = RandomizedSearchCV(\n",
    "    svm,\n",
    "    param_distributions=param_dist_svm,\n",
    "    n_iter=5,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Entrenando SVM con RandomizedSearchCV (versión rápida)...\")\n",
    "random_search_svm.fit(X_train_small, y_train_small)\n",
    "\n",
    "print(\"Mejores hiperparámetros SVM:\", random_search_svm.best_params_)\n",
    "print(\"Mejor score de validación cruzada:\", random_search_svm.best_score_)\n",
    "\n",
    "# Evaluación en el conjunto de validación hold-out\n",
    "best_svm = random_search_svm.best_estimator_\n",
    "val_score_svm = best_svm.score(X_val, y_val)\n",
    "print(\"Accuracy en conjunto de validación:\", val_score_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e4878b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando búsqueda de hiperparámetros (muestra)...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "\n",
      "Mejores hiperparámetros encontrados (muestra): {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 40, 'criterion': 'gini', 'bootstrap': False}\n",
      "Mejor score CV en la muestra: 0.9897998791598223\n",
      "\n",
      "Reentrenando Random Forest con mejor configuración en todo X_train...\n",
      "Accuracy en conjunto de validación: 0.9962\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Usar una muestra para tuning (más rápido). Ajusta n_samples según tu RAM/tiempo.\n",
    "n_samples = 5000\n",
    "X_tune = X_train.sample(n=min(n_samples, len(X_train)), random_state=42)\n",
    "y_tune = y_train.loc[X_tune.index]\n",
    "\n",
    "# Espacio de búsqueda razonable y no excesivo\n",
    "param_dist_rf = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [None, 20, 30, 40],\n",
    "    'max_features': ['sqrt', 'log2', 0.5],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "# RandomizedSearchCV (cv=3 para acelerar)\n",
    "rs = RandomizedSearchCV(\n",
    "    rf,\n",
    "    param_distributions=param_dist_rf,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Iniciando búsqueda de hiperparámetros (muestra)...\")\n",
    "rs.fit(X_tune, y_tune)\n",
    "\n",
    "print(\"\\nMejores hiperparámetros encontrados (muestra):\", rs.best_params_)\n",
    "print(\"Mejor score CV en la muestra:\", rs.best_score_)\n",
    "\n",
    "# Reentrenar el modelo con los mejores hiperparámetros sobre TODO el X_train\n",
    "best_params = rs.best_params_\n",
    "best_rf = RandomForestClassifier(**best_params, random_state=42, n_jobs=-1)\n",
    "\n",
    "print(\"\\nReentrenando Random Forest con mejor configuración en todo X_train...\")\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluación en hold-out\n",
    "val_score_rf = best_rf.score(X_val, y_val)\n",
    "print(f\"Accuracy en conjunto de validación: {val_score_rf:.4f}\")\n",
    "\n",
    "# Guardar el objeto de búsqueda si se necesita más tarde\n",
    "random_search_rf = rs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e954b747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: KNN -> models\\KNN_20251203_124947.joblib\n",
      "Guardado: GaussianNB -> models\\GaussianNB_20251203_124947.joblib\n",
      "Guardado: LogisticRegression -> models\\LogisticRegression_20251203_124947.joblib\n",
      "Guardado: DecisionTree -> models\\DecisionTree_20251203_124947.joblib\n",
      "Guardado: SVM -> models\\SVM_20251203_124947.joblib\n",
      "Guardado: RandomForest -> models\\RandomForest_20251203_124947.joblib\n",
      "Guardado: best_models_dict -> models\\best_models_dict_20251203_124947.joblib\n",
      "\n",
      "Resumen de archivos guardados:\n",
      " - KNN: models\\KNN_20251203_124947.joblib\n",
      " - GaussianNB: models\\GaussianNB_20251203_124947.joblib\n",
      " - LogisticRegression: models\\LogisticRegression_20251203_124947.joblib\n",
      " - DecisionTree: models\\DecisionTree_20251203_124947.joblib\n",
      " - SVM: models\\SVM_20251203_124947.joblib\n",
      " - RandomForest: models\\RandomForest_20251203_124947.joblib\n",
      " - best_models_dict: models\\best_models_dict_20251203_124947.joblib\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "# Guardar los mejores modelos (KNN, GaussianNB, LogisticRegression, DecisionTree, SVM, RandomForest)\n",
    "\n",
    "models_dir = Path(\"models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "models_to_save = {\n",
    "    'KNN': globals().get('best_knn'),\n",
    "    'GaussianNB': globals().get('best_gnb'),\n",
    "    'LogisticRegression': globals().get('best_lr'),\n",
    "    'DecisionTree': globals().get('best_dt'),\n",
    "    'SVM': globals().get('best_svm'),\n",
    "    'RandomForest': globals().get('best_rf'),\n",
    "    'best_models_dict': globals().get('best_models')\n",
    "}\n",
    "\n",
    "saved_files = {}\n",
    "for name, model in models_to_save.items():\n",
    "    if model is None:\n",
    "        print(f\"Advertencia: '{name}' no existe en el entorno y no se guardará.\")\n",
    "        continue\n",
    "    fname = models_dir / f\"{name}_{timestamp}.joblib\"\n",
    "    joblib.dump(model, fname)\n",
    "    saved_files[name] = str(fname)\n",
    "    print(f\"Guardado: {name} -> {fname}\")\n",
    "\n",
    "# Mostrar resumen\n",
    "print(\"\\nResumen de archivos guardados:\")\n",
    "for k, v in saved_files.items():\n",
    "    print(f\" - {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64de7a7c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Paso 5: Crear una tabla resumen de los mejores modelos\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "Después de entrenar y optimizar todos los clasificadores, debes construir una **tabla resumen en formato Markdown** que incluya:\n",
    "\n",
    "- El **nombre del modelo**\n",
    "- Los **hiperparámetros óptimos** encontrados mediante validación cruzada\n",
    "\n",
    "### Requisitos:\n",
    "\n",
    "- La tabla debe estar escrita en formato **Markdown**.\n",
    "- Cada fila debe corresponder a uno de los modelos evaluados.\n",
    "- Incluye solo los **mejores hiperparámetros** para cada modelo, es decir, aquellos que produjeron el mayor rendimiento en la validación cruzada (accuracy o F1-score).\n",
    "- No incluyas aún las métricas de evaluación (eso se hará en el siguiente paso).\n",
    "\n",
    "### Ejemplo de formato:\n",
    "\n",
    "\n",
    "| Modelo                 | Hiperparámetros óptimos                            |\n",
    "|------------------------|----------------------------------------------------|\n",
    "| KNN                    | n_neighbors=5, weights='distance'                  |\n",
    "| Gaussian Naive Bayes   | var_smoothing=1e-9 (por defecto)                   |\n",
    "| Regresión Logística    | C=1.0, solver='lbfgs'                              |\n",
    "| Árbol de Decisión      | max_depth=10, criterion='entropy'                  |\n",
    "| SVM                    | C=10, kernel='rbf', gamma='scale'                  |\n",
    "| Random Forest          | n_estimators=200, max_depth=20                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db06abe",
   "metadata": {},
   "source": [
    "# tu tabla de resultados aquí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5c94c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Modelo | Hiperparámetros óptimos |\n",
       "|---|---|\n",
       "| K-Nearest Neighbors (KNN) | n_neighbors=3, weights='distance', metric='euclidean' |\n",
       "| Gaussian Naive Bayes | var_smoothing=np.float64(0.01) |\n",
       "| Regresión Logística | clf__C=np.float64(32.90344562312671), clf__penalty='l2', clf__solver='lbfgs' |\n",
       "| Árbol de Decisión | criterion='entropy', min_samples_split=np.int64(7), min_samples_leaf=np.int64(3), splitter='random', max_depth=None |\n",
       "| Support Vector Machines (SVM) | C=0.1, kernel='linear', gamma='scale' |\n",
       "| Random Forest | n_estimators=300, max_depth=40, max_features='sqrt', criterion='gini', bootstrap=False, min_samples_split=2, min_samples_leaf=1 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tabla guardada en: models\\best_models_summary.md\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Paso 5: Crear y mostrar la tabla resumen (Markdown) con los mejores hiperparámetros\n",
    "\n",
    "# Construir la tabla a partir de summary_table si ya existe, sino desde los modelos guardados en memoria\n",
    "if 'summary_table' in globals():\n",
    "    md_table = summary_table\n",
    "else:\n",
    "    bm = globals().get('best_models', {})\n",
    "    rows = []\n",
    "\n",
    "    # KNN\n",
    "    knn = bm.get('KNN') or globals().get('best_knn')\n",
    "    if knn is not None:\n",
    "        rows.append((\n",
    "            \"K-Nearest Neighbors (KNN)\",\n",
    "            f\"n_neighbors={getattr(knn,'n_neighbors',None)}, weights='{getattr(knn,'weights',None)}', metric='{getattr(knn,'metric',None)}'\"\n",
    "        ))\n",
    "\n",
    "    # GaussianNB\n",
    "    gnb = bm.get('GaussianNB') or globals().get('best_gnb')\n",
    "    if gnb is not None:\n",
    "        rows.append((\"Gaussian Naive Bayes\", f\"var_smoothing={getattr(gnb,'var_smoothing',None)}\"))\n",
    "\n",
    "    # Logistic Regression (pipeline)\n",
    "    lr = bm.get('LogisticRegression') or globals().get('best_lr')\n",
    "    if lr is not None:\n",
    "        clf = None\n",
    "        try:\n",
    "            clf = lr.named_steps.get('clf')  # pipeline\n",
    "        except Exception:\n",
    "            clf = lr\n",
    "        if clf is not None:\n",
    "            rows.append((\n",
    "                \"Regresión Logística\",\n",
    "                f\"clf__C={getattr(clf,'C',None)}, clf__penalty='{getattr(clf,'penalty',None)}', clf__solver='{getattr(clf,'solver',None)}'\"\n",
    "            ))\n",
    "\n",
    "    # Decision Tree\n",
    "    dt = bm.get('DecisionTree') or globals().get('best_dt') or globals().get('dt')\n",
    "    if dt is not None:\n",
    "        rows.append((\n",
    "            \"Árbol de Decisión\",\n",
    "            f\"criterion='{getattr(dt,'criterion',None)}', min_samples_split={getattr(dt,'min_samples_split',None)}, \"\n",
    "            f\"min_samples_leaf={getattr(dt,'min_samples_leaf',None)}, splitter='{getattr(dt,'splitter',None)}', max_depth={getattr(dt,'max_depth',None)}\"\n",
    "        ))\n",
    "\n",
    "    # SVM\n",
    "    svm = bm.get('SVM') or globals().get('best_svm')\n",
    "    if svm is not None:\n",
    "        rows.append((\n",
    "            \"Support Vector Machines (SVM)\",\n",
    "            f\"C={getattr(svm,'C',None)}, kernel='{getattr(svm,'kernel',None)}', gamma='{getattr(svm,'gamma',None)}'\"\n",
    "        ))\n",
    "\n",
    "    # Random Forest\n",
    "    rf = bm.get('RandomForest') or globals().get('best_rf') or globals().get('best_rf_full')\n",
    "    if rf is not None:\n",
    "        rows.append((\n",
    "            \"Random Forest\",\n",
    "            f\"n_estimators={getattr(rf,'n_estimators',None)}, max_depth={getattr(rf,'max_depth',None)}, \"\n",
    "            f\"max_features='{getattr(rf,'max_features',None)}', criterion='{getattr(rf,'criterion',None)}', \"\n",
    "            f\"bootstrap={getattr(rf,'bootstrap',None)}, min_samples_split={getattr(rf,'min_samples_split',None)}, min_samples_leaf={getattr(rf,'min_samples_leaf',None)}\"\n",
    "        ))\n",
    "\n",
    "    # Montar la tabla Markdown\n",
    "    md_table = \"| Modelo | Hiperparámetros óptimos |\\n|---|---|\\n\"\n",
    "    for name, hp in rows:\n",
    "        md_table += f\"| {name} | {hp} |\\n\"\n",
    "\n",
    "# Mostrar la tabla en el notebook (Markdown) si es posible, y guardarla en models/best_models_summary.md\n",
    "try:\n",
    "    display(Markdown(md_table))\n",
    "except Exception:\n",
    "    print(md_table)\n",
    "\n",
    "models_dir = Path(\"models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "summary_path = models_dir / \"best_models_summary.md\"\n",
    "summary_path.write_text(md_table)\n",
    "\n",
    "print(f\"\\nTabla guardada en: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8951e6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Paso 6: Preparar los datos finales para evaluación\n",
    "\n",
    "**Objetivo:**\n",
    "Cargar el dataset de entrenamiento y prueba, limpiar las columnas innecesarias, ajustar los valores de señal, y dejar los datos listos para probar los modelos entrenados.\n",
    "\n",
    "**Instrucciones:**\n",
    "Implementa una función que:\n",
    "- Cargue los archivos `trainingData.csv` y `validationData.csv`\n",
    "- Elimine las columnas irrelevantes (`LONGITUDE`, `LATITUDE`, `SPACEID`, `RELATIVEPOSITION`, `USERID`, `PHONEID`, `TIMESTAMP`)\n",
    "- Reemplace los valores `100` por `-100` en las columnas `WAP001` a `WAP520`\n",
    "- Separe las características (`X`) y la variable objetivo (`FLOOR`)\n",
    "- Devuelva los conjuntos `X_train`, `X_test`, `y_train`, `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8787a394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def load_prepare_ujiindoorloc(training_path=None, validation_path=None,\n",
    "                              drop_cols=None, wap_prefix='WAP', n_waps=520):\n",
    "    \"\"\"\n",
    "    Carga y prepara los datasets trainingData.csv y validationData.csv:\n",
    "    - elimina columnas irrelevantes (drop_cols si se proporciona, sino usa cols_to_drop del entorno si existe)\n",
    "    - reemplaza 100 por -100 en columnas WAP001..WAP{n_waps}\n",
    "    - separa X (WAPs) e y (FLOOR)\n",
    "    Devuelve: X_train, X_test, y_train, y_test (pandas DataFrame/Series)\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtener lista de columnas a eliminar (usar variable existente si está en el notebook)\n",
    "    if drop_cols is None:\n",
    "        drop_cols = globals().get('cols_to_drop',\n",
    "                                 ['LONGITUDE', 'LATITUDE', 'SPACEID', 'RELATIVEPOSITION', 'USERID', 'PHONEID', 'TIMESTAMP'])\n",
    "\n",
    "    # función auxiliar para localizar archivos\n",
    "    def _find_file(fname, provided):\n",
    "        if provided:\n",
    "            p = Path(provided)\n",
    "            if p.exists():\n",
    "                return p\n",
    "            raise FileNotFoundError(f\"Archivo especificado no existe: {provided}\")\n",
    "        # buscar en candidate_paths si existe en el entorno\n",
    "        cand = globals().get('candidate_paths', [])\n",
    "        for p in cand:\n",
    "            try:\n",
    "                if Path(p).exists() and Path(p).name.lower() == fname.lower():\n",
    "                    return Path(p)\n",
    "            except Exception:\n",
    "                continue\n",
    "        # rutas comunes\n",
    "        common = [Path(fname), Path('data')/fname, Path('datasets/UJIIndoorLoc')/fname, Path('./')/fname]\n",
    "        for p in common:\n",
    "            if p.exists():\n",
    "                return p\n",
    "        raise FileNotFoundError(f\"No se encontró {fname} en rutas comunes. Pasa la ruta como parámetro.\")\n",
    "\n",
    "    train_fp = _find_file('trainingData.csv', training_path)\n",
    "    val_fp = _find_file('validationData.csv', validation_path)\n",
    "\n",
    "    df_train = pd.read_csv(train_fp)\n",
    "    df_val = pd.read_csv(val_fp)\n",
    "\n",
    "    # eliminar columnas irrelevantes (si están)\n",
    "    df_train = df_train.drop(columns=drop_cols, errors='ignore').copy()\n",
    "    df_val = df_val.drop(columns=drop_cols, errors='ignore').copy()\n",
    "\n",
    "    # determinar columnas WAP presentes\n",
    "    wap_cols = [f\"{wap_prefix}{str(i).zfill(3)}\" for i in range(1, n_waps + 1)]\n",
    "    wap_cols = [c for c in wap_cols if c in df_train.columns and c in df_val.columns]\n",
    "    if not wap_cols:\n",
    "        raise KeyError(\"No se detectaron columnas WAP en los archivos proporcionados.\")\n",
    "\n",
    "    # reemplazar 100 -> -100 (ausencia de señal)\n",
    "    df_train.loc[:, wap_cols] = df_train.loc[:, wap_cols].replace(100, -100)\n",
    "    df_val.loc[:, wap_cols] = df_val.loc[:, wap_cols].replace(100, -100)\n",
    "\n",
    "    # verificar target\n",
    "    if 'FLOOR' not in df_train.columns or 'FLOOR' not in df_val.columns:\n",
    "        raise KeyError(\"La columna 'FLOOR' no se encontró en uno de los datasets.\")\n",
    "\n",
    "    X_train = df_train[wap_cols].copy()\n",
    "    y_train = df_train['FLOOR'].copy()\n",
    "    X_test = df_val[wap_cols].copy()\n",
    "    y_test = df_val['FLOOR'].copy()\n",
    "\n",
    "    # breve resumen\n",
    "    print(f\"Archivos cargados: {train_fp.name} ({df_train.shape}), {val_fp.name} ({df_val.shape})\")\n",
    "    print(f\"WAPs usadas: {len(wap_cols)} (primeras 5: {wap_cols[:5]})\")\n",
    "    print(f\"Rango RSSI train: {X_train.min().min()} .. {X_train.max().max()}\")\n",
    "    print(f\"Rango RSSI test:  {X_test.min().min()} .. {X_test.max().max()}\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e1611e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Paso 7: Evaluar modelos optimizados en el conjunto de prueba\n",
    "\n",
    "**Objetivo:**\n",
    "Evaluar el rendimiento real de los modelos optimizados usando el conjunto de prueba (`X_test`, `y_test`), previamente separado. Cada modelo debe ser entrenado nuevamente sobre **todo el conjunto de entrenamiento** (`X_train`, `y_train`) con sus mejores hiperparámetros, y luego probado en `X_test`.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. Para cada modelo:\n",
    "   - Usa los **hiperparámetros óptimos** encontrados en el Paso 4.\n",
    "   - Entrena el modelo con `X_train` y `y_train`.\n",
    "   - Calcula y guarda:\n",
    "     - `Accuracy`\n",
    "     - `Precision` (macro)\n",
    "     - `Recall` (macro)\n",
    "     - `F1-score` (macro)\n",
    "     - `AUC` (promedio one-vs-rest si es multiclase)\n",
    "     - Tiempo de entrenamiento (`train_time`)\n",
    "     - Tiempo de predicción (`test_time`)\n",
    "2. Muestra todos los resultados en una **tabla comparativa**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d3fca53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos cargados: trainingData.csv ((19937, 522)), validationData.csv ((1111, 522))\n",
      "WAPs usadas: 520 (primeras 5: ['WAP001', 'WAP002', 'WAP003', 'WAP004', 'WAP005'])\n",
      "Rango RSSI train: -104 .. 0\n",
      "Rango RSSI test:  -102 .. -34\n",
      "Evaluando: KNN\n",
      "Evaluando: GaussianNB\n",
      "Evaluando: LogisticRegression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kenet\\Documents\\keneth_bracamonte_clase_IA\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando: DecisionTree\n",
      "Evaluando: SVM\n",
      "Evaluando: RandomForest\n",
      "\n",
      "Resultados de evaluación (ordenados por accuracy):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_time_s</th>\n",
       "      <th>test_time_s</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>auc_ovr_macro</th>\n",
       "      <th>saved_model_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>1.775390</td>\n",
       "      <td>0.048880</td>\n",
       "      <td>0.911791</td>\n",
       "      <td>0.925350</td>\n",
       "      <td>0.885617</td>\n",
       "      <td>0.900750</td>\n",
       "      <td>0.987618</td>\n",
       "      <td>models\\RandomForest_final.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.035920</td>\n",
       "      <td>0.364792</td>\n",
       "      <td>0.907291</td>\n",
       "      <td>0.920505</td>\n",
       "      <td>0.900514</td>\n",
       "      <td>0.907909</td>\n",
       "      <td>0.946595</td>\n",
       "      <td>models\\KNN_final.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>2.478427</td>\n",
       "      <td>0.004609</td>\n",
       "      <td>0.891989</td>\n",
       "      <td>0.873486</td>\n",
       "      <td>0.897204</td>\n",
       "      <td>0.882796</td>\n",
       "      <td>0.970602</td>\n",
       "      <td>models\\LogisticRegression_final.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM</td>\n",
       "      <td>2.799414</td>\n",
       "      <td>0.064311</td>\n",
       "      <td>0.883888</td>\n",
       "      <td>0.873658</td>\n",
       "      <td>0.890209</td>\n",
       "      <td>0.880999</td>\n",
       "      <td>0.956739</td>\n",
       "      <td>models\\SVM_final.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.336492</td>\n",
       "      <td>0.002774</td>\n",
       "      <td>0.779478</td>\n",
       "      <td>0.773922</td>\n",
       "      <td>0.792129</td>\n",
       "      <td>0.776083</td>\n",
       "      <td>0.887806</td>\n",
       "      <td>models\\DecisionTree_final.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.151523</td>\n",
       "      <td>0.028804</td>\n",
       "      <td>0.775878</td>\n",
       "      <td>0.696839</td>\n",
       "      <td>0.799904</td>\n",
       "      <td>0.716969</td>\n",
       "      <td>0.936128</td>\n",
       "      <td>models\\GaussianNB_final.joblib</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  train_time_s  test_time_s  accuracy  precision_macro  \\\n",
       "0        RandomForest      1.775390     0.048880  0.911791         0.925350   \n",
       "1                 KNN      0.035920     0.364792  0.907291         0.920505   \n",
       "2  LogisticRegression      2.478427     0.004609  0.891989         0.873486   \n",
       "3                 SVM      2.799414     0.064311  0.883888         0.873658   \n",
       "4        DecisionTree      0.336492     0.002774  0.779478         0.773922   \n",
       "5          GaussianNB      0.151523     0.028804  0.775878         0.696839   \n",
       "\n",
       "   recall_macro  f1_macro  auc_ovr_macro  \\\n",
       "0      0.885617  0.900750       0.987618   \n",
       "1      0.900514  0.907909       0.946595   \n",
       "2      0.897204  0.882796       0.970602   \n",
       "3      0.890209  0.880999       0.956739   \n",
       "4      0.792129  0.776083       0.887806   \n",
       "5      0.799904  0.716969       0.936128   \n",
       "\n",
       "                         saved_model_path  \n",
       "0        models\\RandomForest_final.joblib  \n",
       "1                 models\\KNN_final.joblib  \n",
       "2  models\\LogisticRegression_final.joblib  \n",
       "3                 models\\SVM_final.joblib  \n",
       "4        models\\DecisionTree_final.joblib  \n",
       "5          models\\GaussianNB_final.joblib  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CSV guardado en: models\\evaluation_results.csv\n",
      "Markdown guardado en: models\\evaluation_results.md\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from pathlib import Path\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Paso 7: Evaluar modelos optimizados en el conjunto de prueba\n",
    "\n",
    "# Cargar y preparar los datos finales (training + validation files)\n",
    "X_train_full, X_test, y_train_full, y_test = load_prepare_ujiindoorloc()\n",
    "\n",
    "# Obtener los mejores modelos/hyperparámetros encontrados\n",
    "models_to_eval = globals().get('best_models') or globals().get('models_dict') or {}\n",
    "\n",
    "results = []\n",
    "models_dir = Path(\"models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "classes = np.unique(y_test)\n",
    "\n",
    "for name, model_template in models_to_eval.items():\n",
    "    print(f\"Evaluando: {name}\")\n",
    "    estimator = clone(model_template)\n",
    "\n",
    "    # Entrenar en todo el conjunto de entrenamiento\n",
    "    t0 = time.time()\n",
    "    estimator.fit(X_train_full, y_train_full)\n",
    "    train_time = time.time() - t0\n",
    "\n",
    "    # Predicción en test (medir tiempo)\n",
    "    t0 = time.time()\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    test_time = time.time() - t0\n",
    "\n",
    "    # Métricas básicas\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    # AUC (manejar predict_proba/decision_function; intentar calibrar si no disponible)\n",
    "    auc = np.nan\n",
    "    try:\n",
    "        y_score = None\n",
    "        if hasattr(estimator, \"predict_proba\"):\n",
    "            y_score = estimator.predict_proba(X_test)\n",
    "        elif hasattr(estimator, \"decision_function\"):\n",
    "            y_score = estimator.decision_function(X_test)\n",
    "        else:\n",
    "            # intentar calibrar para obtener probabilidades\n",
    "            try:\n",
    "                calib = CalibratedClassifierCV(base_estimator=clone(model_template), cv=3)\n",
    "                calib.fit(X_train_full, y_train_full)\n",
    "                y_score = calib.predict_proba(X_test)\n",
    "            except Exception:\n",
    "                y_score = None\n",
    "\n",
    "        if y_score is not None:\n",
    "            # Caso binario: y_score 1D o (n_samples,1)\n",
    "            if y_score.ndim == 1 or (y_score.ndim == 2 and y_score.shape[1] == 1):\n",
    "                # asegurar array 1D\n",
    "                y_sc = y_score.ravel()\n",
    "                auc = roc_auc_score(y_test, y_sc)\n",
    "            else:\n",
    "                # multiclass: binarizar y_test y calcular OVR macro\n",
    "                y_test_bin = label_binarize(y_test, classes=classes)\n",
    "                # si label_binarize devuelve una sola columna para binario, tratarlo\n",
    "                if y_test_bin.ndim == 1 or (y_test_bin.ndim == 2 and y_test_bin.shape[1] == 1):\n",
    "                    auc = roc_auc_score(y_test, y_score.ravel())\n",
    "                else:\n",
    "                    auc = roc_auc_score(y_test_bin, y_score, multi_class='ovr', average='macro')\n",
    "    except Exception:\n",
    "        auc = np.nan\n",
    "\n",
    "    # Guardar modelo entrenado\n",
    "    saved_path = models_dir / f\"{name}_final.joblib\"\n",
    "    joblib.dump(estimator, saved_path)\n",
    "\n",
    "    results.append({\n",
    "        'model': name,\n",
    "        'train_time_s': train_time,\n",
    "        'test_time_s': test_time,\n",
    "        'accuracy': acc,\n",
    "        'precision_macro': prec,\n",
    "        'recall_macro': rec,\n",
    "        'f1_macro': f1,\n",
    "        'auc_ovr_macro': auc,\n",
    "        'saved_model_path': str(saved_path)\n",
    "    })\n",
    "\n",
    "# Construir dataframe de resultados, ordenar por accuracy descendente y guardarlo\n",
    "results_df = pd.DataFrame(results).sort_values(by='accuracy', ascending=False).reset_index(drop=True)\n",
    "results_csv = models_dir / \"evaluation_results.csv\"\n",
    "results_md = models_dir / \"evaluation_results.md\"\n",
    "\n",
    "results_df.to_csv(results_csv, index=False)\n",
    "\n",
    "# Crear tabla Markdown simple\n",
    "md = \"| modelo | train_time_s | test_time_s | accuracy | precision_macro | recall_macro | f1_macro | auc_ovr_macro | saved_model_path |\\n\"\n",
    "md += \"|---|---:|---:|---:|---:|---:|---:|---:|---|\\n\"\n",
    "for _, r in results_df.iterrows():\n",
    "    auc_val = f\"{r['auc_ovr_macro']:.6f}\" if not pd.isna(r['auc_ovr_macro']) else 'NaN'\n",
    "    md += f\"| {r['model']} | {r['train_time_s']:.3f} | {r['test_time_s']:.3f} | {r['accuracy']:.4f} | {r['precision_macro']:.4f} | {r['recall_macro']:.4f} | {r['f1_macro']:.4f} | {auc_val} | {r['saved_model_path']} |\\n\"\n",
    "results_md.write_text(md)\n",
    "\n",
    "# Mostrar resumen\n",
    "print(\"\\nResultados de evaluación (ordenados por accuracy):\")\n",
    "display(results_df)\n",
    "\n",
    "print(f\"\\nCSV guardado en: {results_csv}\")\n",
    "print(f\"Markdown guardado en: {results_md}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10347fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tu código aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcf813b",
   "metadata": {},
   "source": [
    "---\n",
    "## Paso 8: Selección y justificación del mejor modelo\n",
    "\n",
    "**Objetivo:**\n",
    "Analizar los resultados obtenidos en el paso anterior y **emitir una conclusión razonada** sobre cuál de los modelos evaluados es el más adecuado para la tarea de predicción del piso en el dataset UJIIndoorLoc.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "- Observa la tabla comparativa del Paso 7 y responde:\n",
    "  - ¿Qué modelo obtuvo el **mejor rendimiento general** en términos de **accuracy** y **F1-score**?\n",
    "  - ¿Qué tan consistente fue su rendimiento en **precision** y **recall**?\n",
    "  - ¿Tiene un **tiempo de entrenamiento o inferencia** excesivamente alto?\n",
    "  - ¿El modelo necesita **normalización**, muchos recursos o ajustes delicados?\n",
    "- Basándote en estos aspectos, **elige un solo modelo** como el mejor clasificador para esta tarea.\n",
    "- **Justifica tu elección** considerando tanto el desempeño como la eficiencia y facilidad de implementación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a61042",
   "metadata": {},
   "source": [
    "# tu respuesta aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a1bbd9",
   "metadata": {},
   "source": [
    "El mejor modelo es Random Forest. \n",
    "\n",
    "Obtuvo el mejor rendimiento en términos de accuracy (0.9118) y F1-score (0.9008). \n",
    "\n",
    "Su precisión y recall fueron consistentes, con valores de 0.9253 y 0.8856, respectivamente. \n",
    "\n",
    "Aunque su tiempo de entrenamiento (1.775s) y predicción (0.049s) es mayor que otros modelos como KNN, sigue siendo razonable. \n",
    "\n",
    "Random Forest ofrece un buen equilibrio entre precisión y robustez, aunque consume más memoria y es menos interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f47b37a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Rúbrica de Evaluación\n",
    "\n",
    "| Paso | Descripción | Puntuación |\n",
    "|------|-------------|------------|\n",
    "| 1 | Cargar y explorar el dataset | 5 |\n",
    "| 2 | Preparar los datos | 5 |\n",
    "| 3 | Preprocesamiento de las señales WiFi | 10 |\n",
    "| 4 | Entrenamiento y optimización de hiperparámetros | 40 |\n",
    "| 5 | Crear una tabla resumen de los mejores modelos | 5 |\n",
    "| 6 | Preparar los datos finales para evaluación | 5 |\n",
    "| 7 | Evaluar modelos optimizados en el conjunto de prueba | 10 |\n",
    "| 8 | Selección y justificación del mejor modelo | 20 |\n",
    "| **Total** | | **100** |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
